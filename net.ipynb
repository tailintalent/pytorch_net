{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from pytorch_net.modules import get_Layer, load_layer_dict\n",
    "from pytorch_net.util import get_activation, get_criterion, get_optimizer, get_full_struct_param, plot_matrices\n",
    "from pytorch_net.util import Early_Stopping, record_data, to_np_array, to_Variable, make_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, target):\n",
    "    \"\"\"Get accuracy from prediction and target\"\"\"\n",
    "    assert len(pred.shape) == len(target.shape) == 1\n",
    "    assert len(pred) == len(target)\n",
    "    pred, target = to_np_array(pred, target)\n",
    "    accuracy = ((pred == target).sum().astype(float) / len(pred))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def flatten(*tensors):\n",
    "    \"\"\"Flatten the tensor except the first dimension\"\"\"\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensors.append(tensor.view(tensor.size(0), -1))\n",
    "    if len(new_tensors) == 1:\n",
    "        new_tensors = new_tensors[0]\n",
    "    return new_tensors\n",
    "\n",
    "\n",
    "def fill_triangular(vec, dim, mode = \"lower\"):\n",
    "    \"\"\"Fill an lower or upper triangular matrices with given vectors\"\"\"\n",
    "    num_examples, size = vec.shape\n",
    "    assert size == dim * (dim + 1) // 2\n",
    "    matrix = torch.zeros(num_examples, dim, dim)\n",
    "    if vec.is_cuda:\n",
    "        matrix = matrix.cuda()\n",
    "    idx = (torch.tril(torch.ones(dim, dim)) == 1).unsqueeze(0)\n",
    "    idx = idx.repeat(num_examples,1,1)\n",
    "    if mode == \"lower\":\n",
    "        matrix[idx] = vec.contiguous().view(-1)\n",
    "    elif mode == \"upper\":\n",
    "        matrix[idx] = vec.contiguous().view(-1)\n",
    "    else:\n",
    "        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def matrix_diag_transform(matrix, fun):\n",
    "    \"\"\"Return the matrices whose diagonal elements have been executed by the function 'fun'.\"\"\"\n",
    "    num_examples = len(matrix)\n",
    "    idx = torch.eye(matrix.size(-1)).byte().unsqueeze(0)\n",
    "    idx = idx.repeat(num_examples, 1, 1)\n",
    "    new_matrix = matrix.clone()\n",
    "    new_matrix[idx] = fun(matrix.diagonal(dim1 = 1, dim2 = 2).contiguous().view(-1))\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def Zip(*data, **kwargs):\n",
    "    \"\"\"Recursive unzipping of data structure\n",
    "    Example: Zip(*[(('a',2), 1), (('b',3), 2), (('c',3), 3), (('d',2), 4)])\n",
    "    ==> [[['a', 'b', 'c', 'd'], [2, 3, 3, 2]], [1, 2, 3, 4]]\n",
    "    Each subtree in the original data must be in the form of a tuple.\n",
    "    In the **kwargs, you can set the function that is applied to each fully unzipped subtree.\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    function = kwargs[\"function\"] if \"function\" in kwargs else None\n",
    "    if len(data) == 1:\n",
    "        return data[0]\n",
    "    data = [list(element) for element in zip(*data)]\n",
    "    for i, element in enumerate(data):\n",
    "        if isinstance(element[0], tuple):\n",
    "            data[i] = Zip(*element, **kwargs)\n",
    "        elif isinstance(element, list):\n",
    "            if function is not None:\n",
    "                data[i] = function(element)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_loss(model, data_loader = None, X = None, y = None, criterion = None, **kwargs):\n",
    "    \"\"\"Get loss using the whole data or data_loader\"\"\"\n",
    "    if data_loader is not None:\n",
    "        assert X is None and y is None\n",
    "        loss_list = []\n",
    "        all_info_dict = {}\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            loss_ele = model.get_loss(X_batch, y_batch, criterion = criterion, **kwargs)\n",
    "            loss_list.append(loss_ele)\n",
    "            for key in model.info_dict:\n",
    "                if key not in all_info_dict:\n",
    "                    all_info_dict[key] = []\n",
    "                all_info_dict[key].append(model.info_dict[key])\n",
    "        for key in model.info_dict:\n",
    "            all_info_dict[key] = np.mean(all_info_dict[key])\n",
    "        loss = torch.stack(loss_list).mean()\n",
    "        model.info_dict = deepcopy(all_info_dict)\n",
    "    else:\n",
    "        assert X is not None and y is not None\n",
    "        loss = model.get_loss(X, y, criterion = criterion, **kwargs)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def plot_model(model, data_loader = None, X = None, y = None):\n",
    "    if data_loader is not None:\n",
    "        assert X is None and y is None\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_all.append(X_batch)\n",
    "            y_all.append(y_batch)\n",
    "        if not isinstance(X_all[0], torch.Tensor):\n",
    "            X_all = Zip(*X_all, function = torch.cat)\n",
    "        else:\n",
    "            X_all = torch.cat(X_all, 0)\n",
    "        y_all = torch.cat(y_all)\n",
    "        model.plot(X_all, y_all)\n",
    "    else:\n",
    "        assert X is not None and y is not None\n",
    "        model.plot(X, y)\n",
    "\n",
    "\n",
    "def prepare_inspection(model, data_loader = None, X = None, y = None, **kwargs):\n",
    "    inspect_functions = kwargs[\"inspect_functions\"] if \"inspect_functions\" in kwargs else None\n",
    "    if data_loader is None:\n",
    "        assert X is not None and y is not None\n",
    "        all_dict_summary = model.prepare_inspection(X, y, **kwargs)\n",
    "        if inspect_functions is not None:\n",
    "            for inspect_function_key, inspect_function in inspect_functions.items():\n",
    "                all_dict_summary[inspect_function_key] = inspect_function(model, X, y, **kwargs)\n",
    "    else:\n",
    "        assert X is None and y is None\n",
    "        all_dict = {}\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            info_dict = model.prepare_inspection(X_batch, y_batch, **kwargs)\n",
    "            for key, item in info_dict.items():\n",
    "                if key not in all_dict:\n",
    "                    all_dict[key] = [item]\n",
    "                else:\n",
    "                    all_dict[key].append(item)\n",
    "            if inspect_functions is not None:\n",
    "                for inspect_function_key, inspect_function in inspect_functions.items():\n",
    "                    inspect_function_result = inspect_function(model, X_batch, y_batch, **kwargs)\n",
    "                    if inspect_function_key not in all_dict:\n",
    "                        all_dict[inspect_function_key] = [inspect_function_result]\n",
    "                    else:\n",
    "                        all_dict[inspect_function_key].append(inspect_function_result)\n",
    "        all_dict_summary = {}\n",
    "        for key, item in all_dict.items():\n",
    "            all_dict_summary[key] = np.mean(all_dict[key])\n",
    "    model.info_dict = all_dict_summary\n",
    "    return all_dict_summary\n",
    "    \n",
    "\n",
    "\n",
    "def train(model, X = None, y = None, train_loader = None, validation_data = None, validation_loader = None, criterion = nn.MSELoss(), inspect_interval = 10, isplot = False, is_cuda = None, **kwargs):\n",
    "    \"\"\"minimal version of training. \"model\" can be a single model or a ordered list of models\"\"\"\n",
    "    def get_regularization(model, **kwargs):\n",
    "        reg_dict = kwargs[\"reg_dict\"] if \"reg_dict\" in kwargs else None\n",
    "        reg = to_Variable([0], is_cuda = is_cuda)\n",
    "        if reg_dict is not None:\n",
    "            for reg_type, reg_coeff in reg_dict.items():\n",
    "                reg = reg + model.get_regularization(source = reg_type, mode = \"L1\", **kwargs) * reg_coeff\n",
    "        return reg\n",
    "    if is_cuda is None:\n",
    "        if X is None and y is None:\n",
    "            assert train_loader is not None\n",
    "            is_cuda = train_loader.dataset.tensors[0].is_cuda\n",
    "        else:\n",
    "            is_cuda = X.is_cuda\n",
    "    \n",
    "    # Optimization kwargs:\n",
    "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 10000\n",
    "    lr = kwargs[\"lr\"] if \"lr\" in kwargs else 5e-3\n",
    "    optim_type = kwargs[\"optim_type\"] if \"optim_type\" in kwargs else \"adam\"\n",
    "    optim_kwargs = kwargs[\"optim_kwargs\"] if \"optim_kwargs\" in kwargs else {}\n",
    "    scheduler_type = kwargs[\"scheduler_type\"] if \"scheduler_type\" in kwargs else \"ReduceLROnPlateau\"\n",
    "    gradient_noise = kwargs[\"gradient_noise\"] if \"gradient_noise\" in kwargs else None\n",
    "\n",
    "    # Inspection kwargs:\n",
    "    inspect_step = kwargs[\"inspect_step\"] if \"inspect_step\" in kwargs else None\n",
    "    inspect_items = kwargs[\"inspect_items\"] if \"inspect_items\" in kwargs else None\n",
    "    inspect_functions = kwargs[\"inspect_functions\"] if \"inspect_functions\" in kwargs else None\n",
    "    if inspect_functions is not None:\n",
    "        for inspect_function_key in inspect_functions:\n",
    "            if inspect_function_key not in inspect_items:\n",
    "                inspect_items.append(inspect_function_key)\n",
    "    inspect_items_interval = kwargs[\"inspect_items_interval\"] if \"inspect_items_interval\" in kwargs else 1000\n",
    "    inspect_image_interval = kwargs[\"inspect_image_interval\"] if \"inspect_image_interval\" in kwargs else None\n",
    "    inspect_loss_precision = kwargs[\"inspect_loss_precision\"] if \"inspect_loss_precision\" in kwargs else 4\n",
    "    callback = kwargs[\"callback\"] if \"callback\" in kwargs else None\n",
    "    \n",
    "    # Saving kwargs:\n",
    "    record_keys = kwargs[\"record_keys\"] if \"record_keys\" in kwargs else [\"loss\"]\n",
    "    filename = kwargs[\"filename\"] if \"filename\" in kwargs else None\n",
    "    if filename is not None:\n",
    "        make_dir(filename)\n",
    "    save_interval = kwargs[\"save_interval\"] if \"save_interval\" in kwargs else None\n",
    "    logdir = kwargs[\"logdir\"] if \"logdir\" in kwargs else None\n",
    "    data_record = {key: [] for key in record_keys}\n",
    "    info_to_save = kwargs[\"info_to_save\"] if \"info_to_save\" in kwargs else None\n",
    "    if info_to_save is not None:\n",
    "        data_record.update(info_to_save)\n",
    "    patience = kwargs[\"patience\"] if \"patience\" in kwargs else 20\n",
    "    if patience is not None:\n",
    "        early_stopping_epsilon = kwargs[\"early_stopping_epsilon\"] if \"early_stopping_epsilon\" in kwargs else 0\n",
    "        early_stopping_monitor = kwargs[\"early_stopping_monitor\"] if \"early_stopping_monitor\" in kwargs else \"loss\"\n",
    "        early_stopping = Early_Stopping(patience = patience, epsilon = early_stopping_epsilon, mode = \"max\" if early_stopping_monitor in [\"accuracy\"] else \"min\")\n",
    "    if logdir is not None:\n",
    "        from pytorch_net.logger import Logger\n",
    "        batch_idx = 0\n",
    "        logger = Logger(logdir)\n",
    "    logimages = kwargs[\"logimages\"] if \"logimages\" in kwargs else None\n",
    "    \n",
    "    if validation_loader is not None:\n",
    "        assert validation_data is None\n",
    "        X_valid, y_valid = None, None\n",
    "    elif validation_data is not None:\n",
    "        X_valid, y_valid = validation_data\n",
    "    else:\n",
    "        X_valid, y_valid = X, y\n",
    "    \n",
    "    # Get original loss:\n",
    "    loss_original = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion, loss_epoch = -1, **kwargs).item()\n",
    "    if \"loss\" in record_keys:\n",
    "        record_data(data_record, [-1, loss_original], [\"iter\", \"loss\"])\n",
    "    if \"param\" in record_keys:\n",
    "        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "    if \"param_grad\" in record_keys:\n",
    "        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "    if filename is not None and save_interval is not None:\n",
    "        record_data(data_record, [{}], [\"model_dict\"])\n",
    "\n",
    "    # Setting up optimizer:\n",
    "    parameters = model.parameters()\n",
    "    num_params = len(list(model.parameters()))\n",
    "    if num_params == 0:\n",
    "        print(\"No parameters to optimize!\")\n",
    "        loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion, loss_epoch = -1, **kwargs).item()\n",
    "        if \"loss\" in record_keys:\n",
    "            record_data(data_record, [0, loss_value], [\"iter\", \"loss\"])\n",
    "        if \"param\" in record_keys:\n",
    "            record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "        if \"param_grad\" in record_keys:\n",
    "            record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "        return loss_original, loss_value, data_record\n",
    "    optimizer = get_optimizer(optim_type, lr, parameters, **optim_kwargs)\n",
    "    \n",
    "    # Setting up gradient noise:\n",
    "    if gradient_noise is not None:\n",
    "        from pytorch_net.util import Gradient_Noise_Scale_Gen\n",
    "        scale_gen = Gradient_Noise_Scale_Gen(epochs = epochs,\n",
    "                                             gamma = gradient_noise[\"gamma\"],  # decay rate\n",
    "                                             eta = gradient_noise[\"eta\"],      # starting variance\n",
    "                                             gradient_noise_interval_epoch = 1,\n",
    "                                            )\n",
    "        gradient_noise_scale = scale_gen.generate_scale(verbose = True)\n",
    "    \n",
    "    # Set up learning rate scheduler:\n",
    "    if scheduler_type is not None:\n",
    "        if scheduler_type == \"ReduceLROnPlateau\":\n",
    "            scheduler_patience = kwargs[\"scheduler_patience\"] if \"scheduler_patience\" in kwargs else 40\n",
    "            scheduler_factor = kwargs[\"scheduler_factor\"] if \"scheduler_factor\" in kwargs else 0.1\n",
    "            scheduler_verbose = kwargs[\"scheduler_verbose\"] if \"scheduler_verbose\" in kwargs else False\n",
    "            scheduler = ReduceLROnPlateau(optimizer, factor = scheduler_factor, patience = scheduler_patience, verbose = scheduler_verbose)\n",
    "        elif scheduler_type == \"LambdaLR\":\n",
    "            scheduler_lr_lambda = kwargs[\"scheduler_lr_lambda\"] if \"scheduler_lr_lambda\" in kwargs else (lambda epoch: 1 / (1 + 0.01 * epoch))\n",
    "            scheduler = LambdaLR(optimizer, lr_lambda = scheduler_lr_lambda)\n",
    "        else:\n",
    "            raise\n",
    "        # First step:\n",
    "        if scheduler_type == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(loss_original)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Initialize inspect_items:\n",
    "    if inspect_items is not None:\n",
    "        print(\"{0}:\".format(-1), end = \"\")\n",
    "        print(\"\\tlr: {0:.3e}\\t loss:{1:.{2}f}\".format(optimizer.param_groups[0][\"lr\"], loss_original, inspect_loss_precision), end = \"\")\n",
    "        info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "        if len(info_dict) > 0:\n",
    "            for item in inspect_items:\n",
    "                if item in info_dict:\n",
    "                    print(\" \\t{0}: {1:.{2}f}\".format(item, info_dict[item], inspect_loss_precision), end = \"\")\n",
    "                    if item in record_keys and item != \"loss\":\n",
    "                        record_data(data_record, [to_np_array(info_dict[item])], [item])\n",
    "        print()\n",
    "\n",
    "    # Initialize logdir:\n",
    "    if logdir is not None:\n",
    "        if logimages is not None:\n",
    "            for tag, image_fun in logimages[\"image_fun\"].items():\n",
    "                image = image_fun(model, logimages[\"X\"], logimages[\"y\"])\n",
    "                logger.log_images(tag, image, -1)\n",
    "\n",
    "    # Training:\n",
    "    to_stop = False\n",
    "    for i in range(epochs + 1):\n",
    "        model.train()\n",
    "        \n",
    "        # Updating gradient noise:\n",
    "        if gradient_noise is not None:\n",
    "            hook_handle_list = []\n",
    "            if i % scale_gen.gradient_noise_interval_epoch == 0:\n",
    "                for h in hook_handle_list:\n",
    "                    h.remove()\n",
    "                hook_handle_list = []\n",
    "                scale_idx = int(i / scale_gen.gradient_noise_interval_epoch)\n",
    "                if scale_idx >= len(gradient_noise_scale):\n",
    "                    current_gradient_noise_scale = gradient_noise_scale[-1]\n",
    "                else:\n",
    "                    current_gradient_noise_scale = gradient_noise_scale[scale_idx]\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    for param in param_group[\"params\"]:\n",
    "                        if param.requires_grad:\n",
    "                            h = param.register_hook(lambda grad: grad + Variable(torch.normal(mean = torch.zeros(grad.size()),\n",
    "                                                                                              std = current_gradient_noise_scale * torch.ones(grad.size()))))\n",
    "                            hook_handle_list.append(h)\n",
    "        \n",
    "        if X is not None and y is not None:\n",
    "            if optim_type != \"LBFGS\":\n",
    "                optimizer.zero_grad()\n",
    "                reg = get_regularization(model, **kwargs)\n",
    "                loss = model.get_loss(X, y, criterion = criterion, loss_epoch = i, **kwargs) + reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                # \"LBFGS\" is a second-order optimization algorithm that requires a slightly different procedure:\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    reg = get_regularization(model, **kwargs)\n",
    "                    loss = model.get_loss(X, y, criterion = criterion, loss_epoch = i, **kwargs) + reg\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                optimizer.step(closure)\n",
    "        else:\n",
    "            for k, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                if optim_type != \"LBFGS\":\n",
    "                    optimizer.zero_grad()\n",
    "                    reg = get_regularization(model, **kwargs)\n",
    "                    loss = model.get_loss(X_batch, y_batch, criterion = criterion, loss_epoch = i, **kwargs) + reg\n",
    "                    loss.backward()\n",
    "                    if logdir is not None:\n",
    "                        batch_idx += 1\n",
    "                        if len(info_dict) > 0:\n",
    "                            for item in inspect_items:\n",
    "                                if item in info_dict:\n",
    "                                    logger.log_scalar(item, info_dict[item], batch_idx)\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        reg = get_regularization(model, **kwargs)\n",
    "                        loss = model.get_loss(X_batch, y_batch, criterion = criterion, loss_epoch = i, **kwargs) + reg\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    if logdir is not None:\n",
    "                        batch_idx += 1\n",
    "                        if len(info_dict) > 0:\n",
    "                            for item in inspect_items:\n",
    "                                if item in info_dict:\n",
    "                                    logger.log_scalar(item, info_dict[item], batch_idx)\n",
    "                    optimizer.step(closure)\n",
    "                \n",
    "                if inspect_step is not None:\n",
    "                    if k % inspect_step == 0:\n",
    "                        print(\"Step {}:   \".format(k), end = \"\")\n",
    "                        print(\"\\tlr: {0:.3e} \\tloss: {1:.{2}f}\".format(optimizer.param_groups[0][\"lr\"], loss.item(), inspect_loss_precision), end = \"\")\n",
    "                        info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "                        if len(info_dict) > 0:\n",
    "                            for item in inspect_items:\n",
    "                                if item in info_dict:\n",
    "                                    print(\" \\t{0}: {1:.{2}f}\".format(item, info_dict[item], inspect_loss_precision), end = \"\")\n",
    "                            print()\n",
    "\n",
    "        if logdir is not None:\n",
    "            # Log values and gradients of the parameters (histogram summary)\n",
    "#             for tag, value in model.named_parameters():\n",
    "#                 tag = tag.replace('.', '/')\n",
    "#                 logger.log_histogram(tag, to_np_array(value), i)\n",
    "#                 logger.log_histogram(tag + '/grad', to_np_array(value.grad), i)\n",
    "            if logimages is not None:\n",
    "                for tag, image_fun in logimages[\"image_fun\"].items():\n",
    "                    image = image_fun(model, logimages[\"X\"], logimages[\"y\"])\n",
    "                    logger.log_images(tag, image, i)\n",
    "\n",
    "        if i % inspect_interval == 0:\n",
    "            model.eval()\n",
    "            loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion, loss_epoch = i, **kwargs).item()\n",
    "            if scheduler_type is not None:\n",
    "                if scheduler_type == \"ReduceLROnPlateau\":\n",
    "                    scheduler.step(loss_value)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "            if callback is not None:\n",
    "                assert callable(callback)\n",
    "                callback(model = model,\n",
    "                         X = X_valid,\n",
    "                         y = y_valid,\n",
    "                         iteration = i,\n",
    "                         loss = loss_value,\n",
    "                        )\n",
    "            if patience is not None:\n",
    "                if early_stopping_monitor == \"loss\":\n",
    "                    to_stop = early_stopping.monitor(loss_value)\n",
    "                else:\n",
    "                    info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "                    to_stop = early_stopping.monitor(info_dict[early_stopping_monitor])\n",
    "            if inspect_items is not None:\n",
    "                if i % inspect_items_interval == 0:\n",
    "                    print(\"{0}:\".format(i), end = \"\")\n",
    "                    print(\"\\tlr: {0:.3e}\\tloss: {1:.{2}f}\".format(optimizer.param_groups[0][\"lr\"], loss_value, inspect_loss_precision), end = \"\")\n",
    "                    info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "                    if len(info_dict) > 0:\n",
    "                        for item in inspect_items:\n",
    "                            if item in info_dict:\n",
    "                                print(\" \\t{0}: {1:.{2}f}\".format(item, info_dict[item], inspect_loss_precision), end = \"\")\n",
    "                                if item in record_keys and item != \"loss\":\n",
    "                                    record_data(data_record, [to_np_array(info_dict[item])], [item])\n",
    "                    if \"loss\" in record_keys:\n",
    "                        record_data(data_record, [i, loss_value], [\"iter\", \"loss\"])\n",
    "                    if \"param\" in record_keys:\n",
    "                        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "                    if \"param_grad\" in record_keys:\n",
    "                        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "                    print()\n",
    "                    try:\n",
    "                        sys.stdout.flush()\n",
    "                    except:\n",
    "                        pass\n",
    "            if isplot and inspect_image_interval is not None and hasattr(model, \"plot\"):\n",
    "                if i % inspect_image_interval == 0:\n",
    "                    if gradient_noise is not None:\n",
    "                        print(\"gradient_noise: {0:.9f}\".format(current_gradient_noise_scale))\n",
    "                    plot_model(model, data_loader = validation_loader, X = X_valid, y = y_valid)\n",
    "        if save_interval is not None:\n",
    "            if i % save_interval == 0:\n",
    "                record_data(data_record, [model.model_dict], [\"model_dict\"])\n",
    "                if filename is not None:\n",
    "                    pickle.dump(data_record, open(filename[:-2] + \"_{0}\".format(i) + \".p\", \"wb\"))\n",
    "        if to_stop:\n",
    "            break\n",
    "\n",
    "    loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion, loss_epoch = epochs, **kwargs).item()\n",
    "    if isplot:\n",
    "        import matplotlib.pylab as plt\n",
    "        for key in data_record:\n",
    "            if key not in [\"iter\", \"model_dict\"]:\n",
    "                if key in [\"accuracy\"]:\n",
    "                    plt.figure(figsize = (8,6))\n",
    "                    plt.plot(data_record[\"iter\"], data_record[key])\n",
    "                    plt.xlabel(\"epoch\")\n",
    "                    plt.ylabel(key)\n",
    "                    plt.title(key)\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    plt.figure(figsize = (8,6))\n",
    "                    plt.semilogy(data_record[\"iter\"], data_record[key])\n",
    "                    plt.xlabel(\"epoch\")\n",
    "                    plt.ylabel(key)\n",
    "                    plt.title(key)\n",
    "                    plt.show()\n",
    "    return loss_original, loss_value, data_record\n",
    "\n",
    "\n",
    "def load_model_dict_net(model_dict, is_cuda = False):\n",
    "    net_type = model_dict[\"type\"]\n",
    "    if net_type == \"MLP\":\n",
    "        return MLP(input_size = model_dict[\"input_size\"],\n",
    "                   struct_param = model_dict[\"struct_param\"],\n",
    "                   W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                   b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                   settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                   is_cuda = is_cuda,\n",
    "                  )\n",
    "    elif net_type == \"Multi_MLP\":\n",
    "        return Multi_MLP(input_size = model_dict[\"input_size\"],\n",
    "                   struct_param = model_dict[\"struct_param\"],\n",
    "                   W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                   b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                   settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                   is_cuda = is_cuda,\n",
    "                  )\n",
    "    elif net_type == \"Branching_Net\":\n",
    "        return Branching_Net(net_base_model_dict = model_dict[\"net_base_model_dict\"],\n",
    "                             net_1_model_dict = model_dict[\"net_1_model_dict\"],\n",
    "                             net_2_model_dict = model_dict[\"net_2_model_dict\"],\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "    elif net_type == \"Fan_in_MLP\":\n",
    "        return Fan_in_MLP(model_dict_branch1=model_dict[\"model_dict_branch1\"],\n",
    "                          model_dict_branch2=model_dict[\"model_dict_branch2\"],\n",
    "                          model_dict_joint=model_dict[\"model_dict_joint\"],\n",
    "                          is_cuda=is_cuda,\n",
    "                         )\n",
    "    elif net_type == \"Net_reparam\":\n",
    "        return Net_reparam(model_dict=model_dict[\"model\"],\n",
    "                           reparam_mode=model_dict[\"reparam_mode\"],\n",
    "                           is_cuda=is_cuda,\n",
    "                          )\n",
    "    elif net_type == \"ConvNet\":\n",
    "        return ConvNet(input_channels = model_dict[\"input_channels\"],\n",
    "                       struct_param = model_dict[\"struct_param\"],\n",
    "                       W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                       b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                       settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                       return_indices = model_dict[\"return_indices\"] if \"return_indices\" in model_dict else False,\n",
    "                       is_cuda = is_cuda,\n",
    "                      )\n",
    "    elif net_type == \"Conv_Autoencoder\":\n",
    "        model = Conv_Autoencoder(input_channels_encoder = model_dict[\"input_channels_encoder\"],\n",
    "                                 input_channels_decoder = model_dict[\"input_channels_decoder\"],\n",
    "                                 struct_param_encoder = model_dict[\"struct_param_encoder\"],\n",
    "                                 struct_param_decoder = model_dict[\"struct_param_decoder\"],\n",
    "                                 settings = model_dict[\"settings\"],\n",
    "                                 is_cuda = is_cuda,\n",
    "                                )\n",
    "        if \"encoder\" in model_dict:\n",
    "            model.encoder.load_model_dict(model_dict[\"encoder\"])\n",
    "        if \"decoder\" in model_dict:\n",
    "            model.decoder.load_model_dict(model_dict[\"decoder\"])\n",
    "        return model\n",
    "    elif model_dict[\"type\"] == \"Conv_Model\":\n",
    "        is_generative = model_dict[\"is_generative\"] if \"is_generative\" in model_dict else False\n",
    "        return Conv_Model(encoder_model_dict = model_dict[\"encoder_model_dict\"] if not is_generative else None,\n",
    "                          core_model_dict = model_dict[\"core_model_dict\"],\n",
    "                          decoder_model_dict = model_dict[\"decoder_model_dict\"],\n",
    "                          latent_size = model_dict[\"latent_size\"],\n",
    "                          is_generative = model_dict[\"is_generative\"] if is_generative else False,\n",
    "                          is_res_block = model_dict[\"is_res_block\"] if \"is_res_block\" in model_dict else False,\n",
    "                          is_cuda = is_cuda,\n",
    "                         )\n",
    "    else:\n",
    "        raise Exception(\"net_type {0} not recognized!\".format(net_type))\n",
    "        \n",
    "\n",
    "def load_model_dict(model_dict, is_cuda = False):\n",
    "    net_type = model_dict[\"type\"]\n",
    "    if net_type not in [\"Model_Ensemble\", \"LSTM\"]:\n",
    "        return load_model_dict_net(model_dict, is_cuda = is_cuda)\n",
    "    elif net_type == \"Model_Ensemble\":\n",
    "        if model_dict[\"model_type\"] == \"MLP\":\n",
    "            model_ensemble = Model_Ensemble(\n",
    "                num_models = model_dict[\"num_models\"],\n",
    "                input_size = model_dict[\"input_size\"],\n",
    "                model_type = model_dict[\"model_type\"],\n",
    "                output_size = model_dict[\"output_size\"],\n",
    "                is_cuda = is_cuda,\n",
    "                # Here we just create some placeholder network. The model will be overwritten in the next steps:\n",
    "                struct_param = [[1, \"Simple_Layer\", {}]],\n",
    "            )\n",
    "        elif model_dict[\"model_type\"] == \"LSTM\":\n",
    "            model_ensemble = Model_Ensemble(\n",
    "                num_models = model_dict[\"num_models\"],\n",
    "                input_size = model_dict[\"input_size\"],\n",
    "                model_type = model_dict[\"model_type\"],\n",
    "                output_size = model_dict[\"output_size\"],\n",
    "                is_cuda = is_cuda,\n",
    "                # Here we just create some placeholder network. The model will be overwritten in the next steps:\n",
    "                hidden_size = 3,\n",
    "                output_struct_param = [[1, \"Simple_Layer\", {}]],\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "        for k in range(model_ensemble.num_models):\n",
    "            setattr(model_ensemble, \"model_{0}\".format(k), load_model_dict(model_dict[\"model_{0}\".format(k)], is_cuda = is_cuda))\n",
    "        return model_ensemble\n",
    "    elif net_type == \"Model_with_Uncertainty\":\n",
    "        return Model_with_Uncertainty(model_pred = load_model_dict(model_dict[\"model_pred\"], is_cuda = is_cuda),\n",
    "                                      model_logstd = load_model_dict(model_dict[\"model_logstd\"], is_cuda = is_cuda))\n",
    "    elif net_type == \"Mixture_Gaussian\":\n",
    "        return load_model_dict_Mixture_Gaussian(model_dict, is_cuda = is_cuda)\n",
    "    else:\n",
    "        raise Exception(\"net_type {0} not recognized!\".format(net_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_Ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Ensemble(nn.Module):\n",
    "    \"\"\"Model_Ensemble is a collection of models with the same architecture \n",
    "       but independent parameters\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models,\n",
    "        input_size,\n",
    "        model_type,\n",
    "        is_cuda = False,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super(Model_Ensemble, self).__init__()\n",
    "        self.num_models = num_models\n",
    "        self.input_size = input_size\n",
    "        self.is_cuda = is_cuda\n",
    "        for i in range(self.num_models):\n",
    "            if model_type == \"MLP\":\n",
    "                model = MLP(input_size = self.input_size, is_cuda = is_cuda, **kwargs)\n",
    "            elif model_type == \"LSTM\":\n",
    "                model = LSTM(input_size = self.input_size, is_cuda = is_cuda, **kwargs)\n",
    "            else:\n",
    "                raise Exception(\"Net_type {0} not recognized!\".format(net_type))\n",
    "            setattr(self, \"model_{0}\".format(i), model)\n",
    "\n",
    "\n",
    "    def get_all_models(self):\n",
    "        return [getattr(self, \"model_{0}\".format(i)) for i in range(self.num_models)]\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_list = []\n",
    "        for i in range(self.num_models):\n",
    "            output = getattr(self, \"model_{0}\".format(i))(input)\n",
    "            if output.size(-1) == 1:\n",
    "                output = output.squeeze(1)\n",
    "            output_list.append(output)\n",
    "        return torch.stack(output_list, -1)\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(self.num_models):\n",
    "            reg = reg + getattr(self, \"model_{0}\".format(k)).get_regularization(\n",
    "                source = source, mode = mode, **kwargs)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def remove_models(self, model_ids):\n",
    "        if not isinstance(model_ids, list):\n",
    "            model_ids = [model_ids]\n",
    "        model_list = []\n",
    "        k = 0\n",
    "        for i in range(self.num_models):\n",
    "            if i not in model_ids:\n",
    "                if k != i:\n",
    "                    setattr(self, \"model_{0}\".format(k), getattr(self, \"model_{0}\".format(i)))\n",
    "                k += 1\n",
    "        num_models_new = k\n",
    "        for i in range(num_models_new, self.num_models):\n",
    "            delattr(self, \"model_{0}\".format(i))\n",
    "        self.num_models = num_models_new\n",
    "\n",
    "\n",
    "    def add_models(self, models):\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "        for i, model in enumerate(models):\n",
    "            setattr(self, \"model_{0}\".format(i + self.num_models), model)\n",
    "        self.num_models += len(models)\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, verbose = False, isplot = False):\n",
    "        W_list_dict = {}\n",
    "        b_list_dict = {}\n",
    "        for i in range(self.num_models):\n",
    "            if verbose:\n",
    "                print(\"\\nmodel {0}:\".format(i))\n",
    "            W_list_dict[i], b_list_dict[i] = getattr(self, \"model_{0}\".format(i)).get_weights_bias(\n",
    "                W_source = W_source, b_source = b_source, verbose = verbose, isplot = isplot)\n",
    "        return W_list_dict, b_list_dict\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_models):\n",
    "            getattr(self, \"model_{0}\".format(k)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "    \n",
    "    \n",
    "    def set_trainable(self, is_trainable):\n",
    "        for i in range(self.num_models):\n",
    "            getattr(self, \"model_{0}\".format(i)).set_trainable(is_trainable)\n",
    "    \n",
    "\n",
    "class Model_with_uncertainty(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_pred,\n",
    "        model_logstd,\n",
    "        ):\n",
    "        super(Model_with_uncertainty, self).__init__()\n",
    "        self.model_pred = model_pred\n",
    "        self.model_logstd = model_logstd\n",
    "        \n",
    "    def forward(self, input, noise_amp = None, **kwargs):\n",
    "        return self.model_pred(input, noise_amp = noise_amp, **kwargs), self.model_logstd(input, **kwargs)\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, noise_amp = None, **kwargs):\n",
    "        pred, log_std = self(input, noise_amp = noise_amp, **kwargs)\n",
    "        return criterion(pred = pred, target = target, log_std = log_std)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        return self.model_pred.get_regularization(source = source, mode = mode, **kwargs) +                 self.model_logstd.get_regularization(source = source, mode = mode, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {}\n",
    "        model_dict[\"type\"] = \"Model_with_Uncertainty\"\n",
    "        model_dict[\"model_pred\"] = self.model_pred.model_dict\n",
    "        model_dict[\"model_logstd\"] = self.model_logstd.model_dict\n",
    "        return model_dict\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        self.model_pred.set_cuda(is_cuda)\n",
    "        self.model_logstd.set_cuda(is_cuda)\n",
    "        \n",
    "    def set_trainable(self, is_trainable):\n",
    "        self.model_pred.set_trainable(is_trainable)\n",
    "        self.model_logstd.set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi_MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = None,          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Multi_MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = deepcopy(settings)\n",
    "        self.num_blocks = len(struct_param)\n",
    "        self.is_cuda = is_cuda\n",
    "        \n",
    "        for i, struct_param_ele in enumerate(struct_param):\n",
    "            input_size_block = input_size if i == 0 else struct_param[i - 1][-1][0]\n",
    "            setattr(self, \"block_{0}\".format(i), MLP(input_size = input_size_block,\n",
    "                                                     struct_param = struct_param_ele,\n",
    "                                                     W_init_list = W_init_list[i] if W_init_list is not None else None,\n",
    "                                                     b_init_list = b_init_list[i] if b_init_list is not None else None,\n",
    "                                                     settings = self.settings[i] if self.settings is not None else {},\n",
    "                                                     is_cuda = self.is_cuda,\n",
    "                                                    ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for i in range(self.num_blocks):\n",
    "            output = getattr(self, \"block_{0}\".format(i))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for i in range(self.num_blocks):\n",
    "            reg = reg + getattr(self, \"block_{0}\".format(i)).get_regularization(mode = mode, source = source)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [getattr(self, \"block_{0}\".format(i)).struct_param for i in range(self.num_blocks)]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Multi_MLP\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"struct_param\"] = self.struct_param\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"settings\"] = deepcopy(self.settings)\n",
    "        model_dict[\"net_type\"] = \"Multi_MLP\"\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_Multi_MLP(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = \"core\", b_source = \"core\"):\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        for i in range(self.num_blocks):\n",
    "            W, b = getattr(self, \"block_{0}\".format(i)).get_weights_bias(W_source = W_source, b_source = b_source)\n",
    "            W_list.append(W)\n",
    "            b_list.append(b)\n",
    "        return deepcopy(W_list), deepcopy(b_list)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        for i in range(self.num_blocks):\n",
    "            getattr(self, \"block_{0}\".format(i)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for i in range(self.num_blocks):\n",
    "            getattr(self, \"block_{0}\".format(i)).set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = {},          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = deepcopy(settings)\n",
    "        self.is_cuda = is_cuda\n",
    "        self.info_dict = {}\n",
    "\n",
    "        self.init_layers(deepcopy(struct_param))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [getattr(self, \"layer_{0}\".format(i)).struct_param for i in range(self.num_layers)]\n",
    "\n",
    "\n",
    "    def init_layers(self, struct_param):\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        for k, layer_struct_param in enumerate(struct_param):\n",
    "            if res_forward:\n",
    "                num_neurons_prev = struct_param[k - 1][0] + self.input_size if k > 0 else self.input_size\n",
    "            else:\n",
    "                num_neurons_prev = struct_param[k - 1][0] if k > 0 else self.input_size\n",
    "            num_neurons = layer_struct_param[0]\n",
    "            W_init = self.W_init_list[k] if self.W_init_list is not None else None\n",
    "            b_init = self.b_init_list[k] if self.b_init_list is not None else None\n",
    "\n",
    "            # Get settings for the current layer:\n",
    "            layer_settings = deepcopy(self.settings) if bool(self.settings) else {}\n",
    "            layer_settings.update(layer_struct_param[2])            \n",
    "\n",
    "            # Construct layer:\n",
    "            layer = get_Layer(layer_type = layer_struct_param[1],\n",
    "                              input_size = num_neurons_prev,\n",
    "                              output_size = num_neurons,\n",
    "                              W_init = W_init,\n",
    "                              b_init = b_init,\n",
    "                              settings = layer_settings,\n",
    "                              is_cuda = self.is_cuda,\n",
    "                             )\n",
    "            setattr(self, \"layer_{0}\".format(k), layer)\n",
    "\n",
    "\n",
    "    def forward(self, input, p_dict = None, **kwargs):\n",
    "        output = input\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        is_res_block = self.settings[\"is_res_block\"] if \"is_res_block\" in self.settings else False\n",
    "        for k in range(len(self.struct_param)):\n",
    "            p_dict_ele = p_dict[k] if p_dict is not None else None\n",
    "            if res_forward and k > 0:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(torch.cat([output, input], -1), p_dict = p_dict_ele)\n",
    "            else:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output, p_dict = p_dict_ele)\n",
    "        if is_res_block:\n",
    "            output = output + input\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(len(self.struct_param)):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            reg = reg + layer.get_regularization(mode = mode, source = source)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def reset_layer(self, layer_id, layer):\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "\n",
    "\n",
    "    def insert_layer(self, layer_id, layer):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if next_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert next_layer.input_size == layer.output_size, \"The inserted layer's output_size {0} must be compatible with next layer_{1}'s input_size {2}!\"\\\n",
    "                    .format(layer.output_size, layer_id + 1, next_layer.input_size)\n",
    "        for i in range(self.num_layers - 1, layer_id - 1, -1):\n",
    "            setattr(self, \"layer_{0}\".format(i + 1), getattr(self, \"layer_{0}\".format(i)))\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "        self.num_layers += 1\n",
    "    \n",
    "    \n",
    "    def remove_layer(self, layer_id):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            num_neurons_prev = self.struct_param[layer_id - 1][0] if layer_id > 0 else self.input_size\n",
    "            replaced_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if replaced_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert replaced_layer.input_size == num_neurons_prev, \\\n",
    "                    \"After deleting layer_{0}, the replaced layer's input_size {1} must be compatible with previous layer's output neurons {2}!\"\\\n",
    "                        .format(layer_id, replaced_layer.input_size, num_neurons_prev)\n",
    "        for i in range(layer_id, self.num_layers - 1):\n",
    "            setattr(self, \"layer_{0}\".format(i), getattr(self, \"layer_{0}\".format(i + 1)))\n",
    "        self.num_layers -= 1\n",
    "\n",
    "\n",
    "    def prune_neurons(self, layer_id, neuron_ids):\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.prune_output_neurons(neuron_ids)\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.prune_input_neurons(neuron_ids)\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "\n",
    "\n",
    "    def add_neurons(self, layer_id, num_neurons, mode = (\"imitation\", \"zeros\")):\n",
    "        if not isinstance(mode, list) and not isinstance(mode, tuple):\n",
    "            mode = (mode, mode)\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.add_output_neurons(num_neurons, mode = mode[0])\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.add_input_neurons(num_neurons, mode = mode[1])\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "\n",
    "\n",
    "    def inspect_operation(self, input, operation_between):\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        output = input\n",
    "        for k in range(*operation_between):\n",
    "            output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "            if res_forward and k > 0:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(torch.cat([output, input], -1))\n",
    "            else:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, layer_ids = None, isplot = False, raise_error = True):\n",
    "        layer_ids = range(len(self.struct_param)) if layer_ids is None else layer_ids\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        if W_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if W_source == \"core\":\n",
    "                        try:\n",
    "                            W, _ = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            W = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"W_source '{0}' not recognized!\".format(W_source))\n",
    "                    W_list.append(W)\n",
    "        \n",
    "        if b_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if b_source == \"core\":\n",
    "                        try:\n",
    "                            _, b = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            b = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"b_source '{0}' not recognized!\".format(b_source))\n",
    "                b_list.append(b)\n",
    "                \n",
    "        if isplot:\n",
    "            if W_source is not None:\n",
    "                print(\"weight {0}:\".format(W_source))\n",
    "                plot_matrices(W_list)\n",
    "            if b_source is not None:\n",
    "                print(\"bias {0}:\".format(b_source))\n",
    "                plot_matrices(b_list)\n",
    "        return W_list, b_list\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"MLP\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"struct_param\"] = get_full_struct_param(self.struct_param, self.settings)\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"settings\"] = deepcopy(self.settings)\n",
    "        model_dict[\"net_type\"] = \"MLP\"\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_layers):\n",
    "            getattr(self, \"layer_{0}\".format(k)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for k in range(self.num_layers):\n",
    "            getattr(self, \"layer_{0}\".format(k)).set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branching_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net_base_model_dict,\n",
    "        net_1_model_dict,\n",
    "        net_2_model_dict,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Branching_Net, self).__init__()\n",
    "        self.net_base = load_model_dict(net_base_model_dict, is_cuda = is_cuda)\n",
    "        self.net_1 = load_model_dict(net_1_model_dict, is_cuda = is_cuda)\n",
    "        self.net_2 = load_model_dict(net_2_model_dict, is_cuda = is_cuda)\n",
    "        self.info_dict = {}\n",
    "    \n",
    "    \n",
    "    def forward(self, X, **kwargs):\n",
    "        shared = self.net_base(X)\n",
    "        shared = shared.max(0, keepdim = True)[0]\n",
    "        return self.net_1(shared)[0], self.net_2(shared)[0]\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weights\", \"bias\"], mode = \"L1\"):\n",
    "        reg = self.net_base.get_regularization(source = source, mode = mode) + \\\n",
    "              self.net_1.get_regularization(source = source, mode = mode) + \\\n",
    "              self.net_2.get_regularization(source = source, mode = mode)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        self.net_base.set_trainable(is_trainable)\n",
    "        self.net_1.set_trainable(is_trainable)\n",
    "        self.net_2.set_trainable(is_trainable)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return deepcopy(self.info_dict)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Branching_Net\"}\n",
    "        model_dict[\"net_base_model_dict\"] = self.net_base.model_dict\n",
    "        model_dict[\"net_1_model_dict\"] = self.net_1.model_dict\n",
    "        model_dict[\"net_2_model_dict\"] = self.net_2.model_dict\n",
    "        return model_dict\n",
    "\n",
    "    \n",
    "class Fan_in_MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dict_branch1,\n",
    "        model_dict_branch2,\n",
    "        model_dict_joint,\n",
    "        is_cuda=False,\n",
    "    ):\n",
    "        super(Fan_in_MLP, self).__init__()\n",
    "        if model_dict_branch1 is not None:\n",
    "            self.net_branch1 = load_model_dict(model_dict_branch1, is_cuda=is_cuda)\n",
    "        else:\n",
    "            self.net_branch1 = None\n",
    "        if model_dict_branch2 is not None:\n",
    "            self.net_branch2 = load_model_dict(model_dict_branch2, is_cuda=is_cuda)\n",
    "        else:\n",
    "            self.net_branch2 = None\n",
    "        self.net_joint = load_model_dict(model_dict_joint, is_cuda=is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "        self.info_dict = {}\n",
    "    \n",
    "    def forward(self, X1, X2, is_outer=False):\n",
    "        if self.net_branch1 is not None:\n",
    "            X1 = self.net_branch1(X1)\n",
    "        if self.net_branch2 is not None:\n",
    "            X2 = self.net_branch2(X2)\n",
    "        # In case there is a sample dimension:\n",
    "        if len(X2.shape) > len(X1.shape):\n",
    "            X1 = X1.unsqueeze(0).expand(X2.shape)\n",
    "        elif len(X1.shape) > len(X2.shape):\n",
    "            X2 = X2.unsqueeze(0).expand(X1.shape)\n",
    "        out = torch.cat([X1, X2], -1)\n",
    "        return self.net_joint(out)\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        X1, X2 = input\n",
    "        y_pred = self(X1, X2)\n",
    "        return criterion(y_pred, target)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        if self.net_branch1 is not None:\n",
    "            reg = reg + self.net_branch1.get_regularization(source=source, mode=mode)\n",
    "        if self.net_branch2 is not None:\n",
    "            reg = reg + self.net_branch2.get_regularization(source=source, mode=mode)\n",
    "        return reg\n",
    "    \n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return deepcopy(self.info_dict)\n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {'type': \"Fan_in_MLP\"}\n",
    "        model_dict[\"model_dict_branch1\"] = self.net_branch1.model_dict\n",
    "        model_dict[\"model_dict_branch2\"] = self.net_branch2.model_dict\n",
    "        model_dict[\"model_dict_joint\"] = self.net_joint.model_dict\n",
    "        return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparameterization toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_reparam(nn.Module):\n",
    "    \"\"\"Module that uses reparameterization to take into two inputs and gets a scaler\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dict,\n",
    "        reparam_mode,\n",
    "        is_cuda=False,\n",
    "        ):\n",
    "        super(Net_reparam, self).__init__()\n",
    "        self.model = load_model_dict(model_dict, is_cuda=is_cuda)\n",
    "        self.reparam_mode = reparam_mode\n",
    "\n",
    "    def forward(self, X, Z, is_outer=False):\n",
    "        \"\"\"\n",
    "        Obtaining single value using reparameterization.\n",
    "\n",
    "        Args:\n",
    "            X shape: [Bx, ...]\n",
    "            Z shape: [S, Bz, Z]\n",
    "            is_outer: whether to use outer product to get a tensor with shape [S, Bz, Bx].\n",
    "        \n",
    "        Returns:\n",
    "            If is_outer==True, return log_prob of shape [S, Bz, Bx]\n",
    "            If is_outer==False, return log_prob of shape [S, Bz]  (where Bz=Bx)\n",
    "        \"\"\"\n",
    "        dist, _ = reparameterize(self.model, X, mode=self.reparam_mode)\n",
    "        if is_outer:\n",
    "            log_prob = dist.log_prob(Z[...,None,:])\n",
    "        else:\n",
    "            log_prob = dist.log_prob(Z)\n",
    "        if self.reparam_mode == 'diag':\n",
    "            log_prob = log_prob.sum(-1)\n",
    "        return log_prob\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        return self.model.get_regularization(source=source, model=mode, **kwargs)\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Net_reparam\"}\n",
    "        model_dict[\"model\"] = self.model.model_dict\n",
    "        model_dict[\"reparam_mode\"] = self.reparam_mode\n",
    "        return model_dict\n",
    "    \n",
    "\n",
    "def reparameterize(model, input, mode = \"full\", size = None):\n",
    "    if mode == \"diag\":\n",
    "        return reparameterize_diagonal(model, input)\n",
    "    elif mode == \"full\":\n",
    "        return reparameterize_full(model, input, size = size)\n",
    "    else:\n",
    "        raise Exception(\"Mode {0} is not valid!\".format(mode))\n",
    "\n",
    "\n",
    "def reparameterize_diagonal(model, input):\n",
    "    mean_logit = model(input)\n",
    "    if isinstance(mean_logit, tuple):\n",
    "        mean_logit = mean_logit[0]\n",
    "    size = int(mean_logit.size(-1) / 2)\n",
    "    mean = mean_logit[:, :size]\n",
    "    std = F.softplus(mean_logit[:, size:], beta = 1)\n",
    "    dist = Normal(mean, std)\n",
    "    return dist, (mean, std)\n",
    "\n",
    "\n",
    "def reparameterize_full(model, input, size = None):\n",
    "    mean_logit = model(input)\n",
    "    if isinstance(mean_logit, tuple):\n",
    "        mean_logit = mean_logit[0]\n",
    "    if size is None:\n",
    "        dim = mean_logit.size(-1)\n",
    "        size = int((np.sqrt(9 + 8 * dim) - 3) / 2)\n",
    "    mean = mean_logit[:, :size]\n",
    "    scale_tril = fill_triangular(mean_logit[:, size:], size)\n",
    "    scale_tril = matrix_diag_transform(scale_tril, F.softplus)\n",
    "    dist = MultivariateNormal(mean, scale_tril = scale_tril)\n",
    "    return dist, (mean, scale_tril)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCellBase(nn.Module):\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if 'bias' in self.__dict__ and self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
    "            s += ', nonlinearity={nonlinearity}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def check_forward_input(self, input):\n",
    "        if input.size(1) != self.input_size:\n",
    "            raise RuntimeError(\n",
    "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
    "                    input.size(1), self.input_size))\n",
    "\n",
    "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
    "        if input.size(0) != hx.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
    "                    input.size(0), hidden_label, hx.size(0)))\n",
    "\n",
    "        if hx.size(1) != self.hidden_size:\n",
    "            raise RuntimeError(\n",
    "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
    "                    hidden_label, hx.size(1), self.hidden_size))\n",
    "\n",
    "\n",
    "class LSTM(RNNCellBase):\n",
    "    \"\"\"a LSTM class\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_struct_param,\n",
    "        output_settings = {},\n",
    "        bias = True,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.W_ih = nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n",
    "        self.W_hh = nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n",
    "        self.output_net = MLP(input_size = self.hidden_size, struct_param = output_struct_param, settings = output_settings, is_cuda = is_cuda)\n",
    "        if bias:\n",
    "            self.b_ih = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "            self.b_hh = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('b_ih', None)\n",
    "            self.register_parameter('b_hh', None)\n",
    "        self.reset_parameters()\n",
    "        self.is_cuda = is_cuda\n",
    "        self.device = torch.device(\"cuda\" if self.is_cuda else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward_one_step(self, input, hx):\n",
    "        self.check_forward_input(input)\n",
    "        self.check_forward_hidden(input, hx[0], '[0]')\n",
    "        self.check_forward_hidden(input, hx[1], '[1]')\n",
    "        return self._backend.LSTMCell(\n",
    "            input, hx,\n",
    "            self.W_ih, self.W_hh,\n",
    "            self.b_ih, self.b_hh,\n",
    "        )\n",
    "    \n",
    "    def forward(self, input, hx = None):\n",
    "        if hx is None:\n",
    "            hx = [torch.randn(input.size(0), self.hidden_size).to(self.device),\n",
    "                  torch.randn(input.size(0), self.hidden_size).to(self.device),\n",
    "                 ]\n",
    "        hhx, ccx = hx\n",
    "        for i in range(input.size(1)):\n",
    "            hhx, ccx = self.forward_one_step(input[:, i], (hhx, ccx))\n",
    "        output = self.output_net(hhx)\n",
    "        return output\n",
    "\n",
    "    def get_regularization(self, source, mode = \"L1\", **kwargs):\n",
    "        if not isinstance(source, list):\n",
    "            source = [source]\n",
    "        reg = self.output_net.get_regularization(source = source, mode = mode)\n",
    "        for source_ele in source:\n",
    "            if source_ele == \"weight\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.W_ih.abs().sum() + self.W_hh.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.W_ih ** 2).sum() + (self.W_hh ** 2).sum()\n",
    "                else:\n",
    "                    raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "            elif source_ele == \"bias\":\n",
    "                if self.bias:\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + self.b_ih.abs().sum() + self.b_hh.abs().sum()\n",
    "                    elif mode == \"L2\":\n",
    "                        reg = reg + (self.b_ih ** 2).sum() + (self.b_hh ** 2).sum()\n",
    "                    else:\n",
    "                        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "            else:\n",
    "                raise Exception(\"source {0} not recognized!\".format(source_ele))\n",
    "        return reg\n",
    "    \n",
    "    def get_weights_bias(self, W_source = None, b_source = None, verbose = False, isplot = False):\n",
    "        W_dict = OrderedDict()\n",
    "        b_dict = OrderedDict()\n",
    "        W_o, b_o = self.output_net.get_weights_bias(W_source = W_source, b_source = b_source)\n",
    "        if W_source == \"core\":\n",
    "            W_dict[\"W_ih\"] = self.W_ih.cpu().detach().numpy()\n",
    "            W_dict[\"W_hh\"] = self.W_hh.cpu().detach().numpy()\n",
    "            W_dict[\"W_o\"] = W_o\n",
    "            if isplot:\n",
    "                print(\"W_ih, W_hh:\")\n",
    "                plot_matrices([W_dict[\"W_ih\"], W_dict[\"W_hh\"]])\n",
    "                print(\"W_o:\")\n",
    "                plot_matrices(W_o)\n",
    "        if self.bias and b_source == \"core\":\n",
    "            b_dict[\"b_ih\"] = self.b_ih.cpu().detach().numpy()\n",
    "            b_dict[\"b_hh\"] = self.b_hh.cpu().detach().numpy()\n",
    "            b_dict[\"b_o\"] = b_o\n",
    "            if isplot:\n",
    "                print(\"b_ih, b_hh:\")\n",
    "                plot_matrices([b_dict[\"b_ih\"], b_dict[\"b_hh\"]])\n",
    "                print(\"b_o:\")\n",
    "                plot_matrices(b_o)\n",
    "        return W_dict, b_dict\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, hx = None, **kwargs):\n",
    "        y_pred = self(input, hx = hx)\n",
    "        return criterion(y_pred, target)\n",
    "    \n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        struct_param,\n",
    "        W_init_list = None,\n",
    "        b_init_list = None,\n",
    "        settings = {},\n",
    "        return_indices = False,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.struct_param = struct_param\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = settings\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.info_dict = {}\n",
    "        self.is_cuda = is_cuda\n",
    "        self.param_available = [\"Conv2d\", \"ConvTranspose2d\", \"BatchNorm2d\", \"Simple_Layer\"]\n",
    "        self.return_indices = return_indices\n",
    "        for i in range(len(self.struct_param)):\n",
    "            if i > 0:\n",
    "                k = 1\n",
    "                while self.struct_param[i - k][0] is None:\n",
    "                    k += 1\n",
    "                num_channels_prev = self.struct_param[i - k][0]\n",
    "            else:\n",
    "                num_channels_prev = input_channels\n",
    "                k = 0\n",
    "            if self.struct_param[i - k][1] == \"Simple_Layer\" and isinstance(num_channels_prev, tuple) and len(num_channels_prev) == 3:\n",
    "                num_channels_prev = num_channels_prev[0]\n",
    "            num_channels = self.struct_param[i][0]\n",
    "            layer_type = self.struct_param[i][1]\n",
    "            layer_settings = self.struct_param[i][2]\n",
    "            if \"layer_input_size\" in layer_settings and isinstance(layer_settings[\"layer_input_size\"], tuple):\n",
    "                num_channels_prev = layer_settings[\"layer_input_size\"][0]\n",
    "            if layer_type == \"Conv2d\":\n",
    "                layer = nn.Conv2d(num_channels_prev, \n",
    "                                  num_channels,\n",
    "                                  kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                  stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else 1,\n",
    "                                  padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                  dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                 )\n",
    "            elif layer_type == \"ConvTranspose2d\":\n",
    "                layer = nn.ConvTranspose2d(num_channels_prev,\n",
    "                                           num_channels,\n",
    "                                           kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                           stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else 1,\n",
    "                                           padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                           output_padding = layer_settings[\"output_padding\"] if \"output_padding\" in layer_settings else 0,\n",
    "                                           dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                          )\n",
    "            elif layer_type == \"Simple_Layer\":\n",
    "                layer = get_Layer(layer_type = layer_type,\n",
    "                                  input_size = layer_settings[\"layer_input_size\"],\n",
    "                                  output_size = num_channels,\n",
    "                                  W_init = W_init_list[i] if self.W_init_list is not None and self.W_init_list[i] is not None else None,\n",
    "                                  b_init = b_init_list[i] if self.b_init_list is not None and self.b_init_list[i] is not None else None,\n",
    "                                  settings = layer_settings,\n",
    "                                  is_cuda = self.is_cuda,\n",
    "                                 )\n",
    "            elif layer_type == \"MaxPool2d\":\n",
    "                layer = nn.MaxPool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                     stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                     padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                     return_indices = layer_settings[\"return_indices\"] if \"return_indices\" in layer_settings else False,\n",
    "                                    )\n",
    "            elif layer_type == \"MaxUnpool2d\":\n",
    "                layer = nn.MaxUnpool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                       stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                       padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                      )\n",
    "            elif layer_type == \"Upsample\":\n",
    "                layer = nn.Upsample(scale_factor = layer_settings[\"scale_factor\"],\n",
    "                                    mode = layer_settings[\"mode\"] if \"mode\" in layer_settings else \"nearest\",\n",
    "                                   )\n",
    "            elif layer_type == \"BatchNorm2d\":\n",
    "                layer = nn.BatchNorm2d(num_features = num_channels)\n",
    "            elif layer_type == \"Dropout2d\":\n",
    "                layer = nn.Dropout2d(p = 0.5)\n",
    "            elif layer_type == \"Flatten\":\n",
    "                layer = Flatten()\n",
    "            else:\n",
    "                raise Exception(\"layer_type {0} not recognized!\".format(layer_type))\n",
    "            \n",
    "            # Initialize using provided initial values:\n",
    "            if self.W_init_list is not None and self.W_init_list[i] is not None and layer_type not in [\"Simple_Layer\"]:\n",
    "                layer.weight.data = torch.FloatTensor(self.W_init_list[i])\n",
    "                layer.bias.data = torch.FloatTensor(self.b_init_list[i])\n",
    "            \n",
    "            setattr(self, \"layer_{0}\".format(i), layer)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    def forward(self, input, indices_list = None, **kwargs):\n",
    "        return self.inspect_operation(input, operation_between = (0, self.num_layers), indices_list = indices_list)\n",
    "    \n",
    "    \n",
    "    def inspect_operation(self, input, operation_between, indices_list = None):\n",
    "        output = input\n",
    "        if indices_list is None:\n",
    "            indices_list = []\n",
    "        start_layer, end_layer = operation_between\n",
    "        if end_layer < 0:\n",
    "            end_layer += self.num_layers\n",
    "        for i in range(start_layer, end_layer):\n",
    "            if \"layer_input_size\" in self.struct_param[i][2]:\n",
    "                output_size_last = output.shape[0]\n",
    "                layer_input_size = self.struct_param[i][2][\"layer_input_size\"]\n",
    "                if not isinstance(layer_input_size, tuple):\n",
    "                    layer_input_size = (layer_input_size,)\n",
    "                output = output.view(-1, *layer_input_size)\n",
    "                assert output.shape[0] == output_size_last, \"output_size reshaped to different length. Check shape!\"\n",
    "            if \"Unpool\" in self.struct_param[i][1]:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output, indices_list.pop(-1))\n",
    "            else:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output)\n",
    "            if isinstance(output_tentative, tuple):\n",
    "                output, indices = output_tentative\n",
    "                indices_list.append(indices)\n",
    "            else:\n",
    "                output = output_tentative\n",
    "            if \"activation\" in self.struct_param[i][2]:\n",
    "                activation = self.struct_param[i][2][\"activation\"]\n",
    "            else:\n",
    "                if \"activation\" in self.settings:\n",
    "                    activation = self.settings[\"activation\"]\n",
    "                else:\n",
    "                    activation = \"linear\"\n",
    "                if \"Pool\" in self.struct_param[i][1] or \"Unpool\" in self.struct_param[i][1] or \"Upsample\" in self.struct_param[i][1]:\n",
    "                    activation = \"linear\"\n",
    "            output = get_activation(activation)(output)\n",
    "        if self.return_indices:\n",
    "            return output, indices_list\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        if self.return_indices:\n",
    "            y_pred = y_pred[0]\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        if not isinstance(source, list):\n",
    "            source = [source]\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] not in self.param_available:\n",
    "                continue\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            for source_ele in source:\n",
    "                if source_ele == \"weight\":\n",
    "                    if self.struct_param[k][1] not in [\"Simple_Layer\"]:\n",
    "                        item = layer.weight\n",
    "                    else:\n",
    "                        item = layer.W_core\n",
    "                elif source_ele == \"bias\":\n",
    "                    if self.struct_param[k][1] not in [\"Simple_Layer\"]:\n",
    "                        item = layer.bias\n",
    "                    else:\n",
    "                        item = layer.b_core\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + item.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (item ** 2).sum()\n",
    "                else:\n",
    "                    raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = \"core\", b_source = \"core\"):\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(to_np_array(layer.W_core))\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(to_np_array(layer.b_core))\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(to_np_array(layer.weight))\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(to_np_array(layer.bias, full_reduce = False))\n",
    "            else:\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(None)\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(None)\n",
    "        return W_list, b_list\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"ConvNet\"}\n",
    "        model_dict[\"net_type\"] = \"ConvNet\"\n",
    "        model_dict[\"input_channels\"] = self.input_channels\n",
    "        model_dict[\"struct_param\"] = self.struct_param\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"return_indices\"] = self.return_indices\n",
    "        return model_dict\n",
    "    \n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        pred_prob = self(X)\n",
    "        if self.return_indices:\n",
    "            pred_prob = pred_prob[0]\n",
    "        pred = pred_prob.max(1)[1]\n",
    "#         self.info_dict[\"accuracy\"] = get_accuracy(pred, y)\n",
    "        return deepcopy(self.info_dict)\n",
    "    \n",
    "    \n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                getattr(self, \"layer_{0}\".format(k)).set_cuda(is_cuda)\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                if is_cuda is True:\n",
    "                    getattr(self, \"layer_{0}\".format(k)).cuda()\n",
    "                else:\n",
    "                    getattr(self, \"layer_{0}\".format(k)).cpu()\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for k in range(self.num_layers):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                layer.set_trainable(is_trainable)\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = is_trainable\n",
    "\n",
    "\n",
    "\n",
    "class Conv_Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_model_dict,\n",
    "        core_model_dict,\n",
    "        decoder_model_dict,\n",
    "        latent_size = 2,\n",
    "        is_generative = True,\n",
    "        is_res_block = True,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        \"\"\"Conv_Model consists of an encoder, a core and a decoder\"\"\"\n",
    "        super(Conv_Model, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.is_generative = is_generative\n",
    "        if not is_generative:\n",
    "            self.encoder = load_model_dict(encoder_model_dict, is_cuda = is_cuda)\n",
    "        self.core = load_model_dict(core_model_dict, is_cuda = is_cuda)\n",
    "        self.decoder = load_model_dict(decoder_model_dict, is_cuda = is_cuda)\n",
    "        self.is_res_block = is_res_block\n",
    "        self.is_cuda = is_cuda\n",
    "        self.info_dict = {}\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        if self.is_generative:\n",
    "            return 1\n",
    "        else:\n",
    "            return len(self.core.model_dict[\"struct_param\"])\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X,\n",
    "        latent = None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        if self.is_generative:\n",
    "            if len(latent.shape) == 1:\n",
    "                latent = latent.repeat(len(X), 1)\n",
    "            latent = self.core(latent)\n",
    "        else:\n",
    "            p_dict = {k: latent if k == 0 else None for k in range(self.num_layers)}\n",
    "            latent = self.encoder(X)\n",
    "            latent = self.core(latent, p_dict = p_dict)\n",
    "        output = self.decoder(latent)\n",
    "        if self.is_res_block:\n",
    "            output = (X + nn.Sigmoid()(output)).clamp(0, 1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def forward_multistep(self, X, latents, isplot = False, num_images = 1):\n",
    "        assert len(latents.shape) == 1\n",
    "        length = int(len(latents) / 2)\n",
    "        output = X\n",
    "        for i in range(length - 1):\n",
    "            latent = latents[i * self.latent_size: (i + 2) * self.latent_size]\n",
    "            output = self(output, latent = latent)\n",
    "            if isplot:\n",
    "                plot_matrices(output[:num_images,0])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_loss(self, X, y, criterion, **kwargs):\n",
    "        return criterion(self(X = X[0], latent = X[1]), y)\n",
    "    \n",
    "    \n",
    "    def plot(self, X, y, num_images = 1):\n",
    "        y_pred = self(X[0], latent = X[1])\n",
    "        idx_list = np.random.choice(len(X[0]), num_images)\n",
    "        for idx in idx_list:\n",
    "            matrix = torch.cat([X[0][idx], y[idx], y_pred[idx]])\n",
    "            plot_matrices(matrix, images_per_row = 8)\n",
    "    \n",
    "    \n",
    "    def get_regularization(self, source = [\"weights\", \"bias\"], mode = \"L1\"):\n",
    "        if self.is_generative:\n",
    "            return self.core.get_regularization(source = source, mode = mode) + \\\n",
    "                    self.decoder.get_regularization(source = source, mode = mode)\n",
    "        else:\n",
    "            return self.encoder.get_regularization(source = source, mode = mode) + \\\n",
    "                    self.core.get_regularization(source = source, mode = mode) + \\\n",
    "                    self.decoder.get_regularization(source = source, mode = mode)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return deepcopy(self.info_dict)\n",
    "    \n",
    "    \n",
    "    def set_trainable(self, is_trainable):\n",
    "        if not self.is_generative:\n",
    "            self.encoder.set_trainable(is_trainable)\n",
    "        self.core.set_trainable(is_trainable)\n",
    "        self.decoder.set_trainable(is_trainable)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Conv_Model\"}\n",
    "        if not self.is_generative:\n",
    "            model_dict[\"encoder_model_dict\"] = self.encoder.model_dict\n",
    "        model_dict[\"latent_size\"] = self.latent_size\n",
    "        model_dict[\"core_model_dict\"] = self.core.model_dict\n",
    "        model_dict[\"decoder_model_dict\"] = self.decoder.model_dict\n",
    "        model_dict[\"is_generative\"] = self.is_generative\n",
    "        model_dict[\"is_res_block\"] = self.is_res_block\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "\n",
    "class Conv_Autoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels_encoder,\n",
    "        input_channels_decoder,\n",
    "        struct_param_encoder,\n",
    "        struct_param_decoder,\n",
    "        latent_size = (1,2),\n",
    "        share_model_among_steps = False,\n",
    "        settings = {},\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        \"\"\"Conv_Autoencoder consists of an encoder and a decoder\"\"\"\n",
    "        super(Conv_Autoencoder, self).__init__()\n",
    "        self.input_channels_encoder = input_channels_encoder\n",
    "        self.input_channels_decoder = input_channels_decoder\n",
    "        self.struct_param_encoder = struct_param_encoder\n",
    "        self.struct_param_decoder = struct_param_decoder\n",
    "        self.share_model_among_steps = share_model_among_steps\n",
    "        self.settings = settings\n",
    "        self.encoder = ConvNet(input_channels = input_channels_encoder, struct_param = struct_param_encoder, settings = settings, is_cuda = is_cuda)\n",
    "        self.decoder = ConvNet(input_channels = input_channels_decoder, struct_param = struct_param_decoder, settings = settings, is_cuda = is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "    \n",
    "    def encode(self, input):\n",
    "        if self.share_model_among_steps:\n",
    "            latent = []\n",
    "            for i in range(input.shape[1]):\n",
    "                latent_step = self.encoder(input[:, i:i+1])\n",
    "                latent.append(latent_step)\n",
    "            return torch.cat(latent, 1)\n",
    "        else:\n",
    "            return self.encoder(input)\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        if self.share_model_among_steps:\n",
    "            latent_size = self.struct_param_encoder[-1][0]\n",
    "            latent = latent.view(latent.size(0), -1, latent_size)\n",
    "            output = []\n",
    "            for i in range(latent.shape[1]):\n",
    "                output_step = self.decoder(latent[:, i].contiguous())\n",
    "                output.append(output_step)\n",
    "            return torch.cat(output, 1)\n",
    "        else:\n",
    "            return self.decoder(latent)\n",
    "    \n",
    "    def set_trainable(self, is_trainable):\n",
    "        self.encoder.set_trainable(is_trainable)\n",
    "        self.decoder.set_trainable(is_trainable)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.decode(self.encode(input))\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        return criterion(self(input), target)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        return self.encoder.get_regularization(source = source, mode = mode) + \\\n",
    "               self.decoder.get_regularization(source = source, mode = mode)\n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Conv_Autoencoder\"}\n",
    "        model_dict[\"net_type\"] = \"Conv_Autoencoder\"\n",
    "        model_dict[\"input_channels_encoder\"] = self.input_channels_encoder\n",
    "        model_dict[\"input_channels_decoder\"] = self.input_channels_decoder\n",
    "        model_dict[\"struct_param_encoder\"] = self.struct_param_encoder\n",
    "        model_dict[\"struct_param_decoder\"] = self.struct_param_decoder\n",
    "        model_dict[\"share_model_among_steps\"] = self.share_model_among_steps\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"encoder\"] = self.encoder.model_dict\n",
    "        model_dict[\"decoder\"] = self.decoder.model_dict\n",
    "        return model_dict\n",
    "    \n",
    "    def load_model_dict(self, model_dict):\n",
    "        model = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(model.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_model_dict,\n",
    "        decoder_model_dict,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = load_model_dict(encoder_model_dict, is_cuda = is_cuda)\n",
    "        self.decoder = load_model_dict(decoder_model_dict, is_cuda = is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "        self.info_dict = {}\n",
    "\n",
    "\n",
    "    def encode(self, X):\n",
    "        Z = self.encoder(X)\n",
    "        latent_size = int(Z.shape[-1] / 2)\n",
    "        mu = Z[..., :latent_size]\n",
    "        logvar = Z[..., latent_size:]\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "\n",
    "    def decode(self, Z):\n",
    "        return self.decoder(Z)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        mu, logvar = self.encode(X)\n",
    "        Z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(Z), mu, logvar\n",
    "\n",
    "\n",
    "    def get_loss(self, X, y = None, **kwargs):\n",
    "        recon_X, mu, logvar = self(X)\n",
    "        BCE = F.binary_cross_entropy(recon_X.view(recon_X.shape[0], -1), X.view(X.shape[0], -1), reduction='sum')\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        loss = (BCE + KLD) / len(X)\n",
    "        self.info_dict[\"KLD\"] = KLD.item() / len(X)\n",
    "        self.info_dict[\"BCE\"] = BCE.item() / len(X)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"VAE\"}\n",
    "        model_dict[\"encoder_model_dict\"] = self.encoder.model_dict\n",
    "        model_dict[\"decoder_model_dict\"] = self.decoder.model_dict\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        return self.encoder.get_regularization(source = source, mode = mode) + self.decoder.get_regularization(source = source, mode = mode)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return deepcopy(self.info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability models:\n",
    "### Mixture of Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixture_Gaussian(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_components,\n",
    "        dim,\n",
    "        param_mode = \"full\",\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Mixture_Gaussian, self).__init__()\n",
    "        self.num_components = num_components\n",
    "        self.dim = dim\n",
    "        self.param_mode = param_mode\n",
    "        self.device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "        self.info_dict = {}\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def initialize(self, model_dict = None, input = None, num_samples = 100, verbose = False):\n",
    "        if input is not None:\n",
    "            neg_log_prob_min = np.inf\n",
    "            loc_init_min = None\n",
    "            scale_init_min = None\n",
    "            for i in range(num_samples):\n",
    "                neg_log_prob, loc_init_list, scale_init_list = self.initialize_ele(input)\n",
    "                if verbose:\n",
    "                    print(\"{0}: neg_log_prob: {1:.4f}\".format(i, neg_log_prob))\n",
    "                if neg_log_prob < neg_log_prob_min:\n",
    "                    neg_log_prob_min = neg_log_prob\n",
    "                    loc_init_min = self.loc_list.detach()\n",
    "                    scale_init_min = self.scale_list.detach()\n",
    "\n",
    "            self.loc_list = nn.Parameter(loc_init_min.to(self.device))\n",
    "            self.scale_list = nn.Parameter(scale_init_min.to(self.device))\n",
    "            print(\"min neg_log_prob: {0:.6f}\".format(to_np_array(neg_log_prob_min)))\n",
    "        else:\n",
    "            if model_dict is None:\n",
    "                self.weight_logits = nn.Parameter((torch.randn(self.num_components) * np.sqrt(2 / (1 + self.dim))).to(self.device))\n",
    "            else:\n",
    "                self.weight_logits = nn.Parameter((torch.FloatTensor(model_dict[\"weight_logits\"])).to(self.device))\n",
    "            if self.param_mode == \"full\": \n",
    "                size = self.dim * (self.dim + 1) // 2\n",
    "            elif self.param_mode == \"diag\":\n",
    "                size = self.dim\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "            if model_dict is None:\n",
    "                self.loc_list = nn.Parameter(torch.randn(self.num_components, self.dim).to(self.device))\n",
    "                self.scale_list = nn.Parameter((torch.randn(self.num_components, size) / self.dim).to(self.device))\n",
    "            else:\n",
    "                self.loc_list = nn.Parameter(torch.FloatTensor(model_dict[\"loc_list\"]).to(self.device))\n",
    "                self.scale_list = nn.Parameter(torch.FloatTensor(model_dict[\"scale_list\"]).to(self.device))\n",
    "\n",
    "\n",
    "    def initialize_ele(self, input):\n",
    "        if self.param_mode == \"full\":\n",
    "            size = self.dim * (self.dim + 1) // 2\n",
    "        elif self.param_mode == \"diag\":\n",
    "            size = self.dim\n",
    "        else:\n",
    "            raise\n",
    "        length = len(input)\n",
    "        self.weight_logits = nn.Parameter(torch.zeros(self.num_components).to(self.device))\n",
    "        self.loc_list = nn.Parameter(input[torch.multinomial(torch.ones(length) / length, self.num_components)].detach())\n",
    "        self.scale_list = nn.Parameter((torch.randn(self.num_components, size).to(self.device) * input.std() / 5).to(self.device))\n",
    "        neg_log_prob = self.get_loss(input)\n",
    "        return neg_log_prob\n",
    "\n",
    "\n",
    "    def prob(self, input):\n",
    "        if len(input.shape) == 1:\n",
    "            input = input.unsqueeze(1)\n",
    "        assert len(input.shape) in [0, 2, 3]\n",
    "        input = input.unsqueeze(-2)\n",
    "        if self.param_mode == \"diag\":\n",
    "            scale_list = F.softplus(self.scale_list)\n",
    "            logits = (- (input - self.loc_list) ** 2 / 2 / scale_list ** 2 - torch.log(scale_list * np.sqrt(2 * np.pi))).sum(-1)\n",
    "        else:\n",
    "            raise\n",
    "        prob = torch.matmul(torch.exp(logits), nn.Softmax(dim = 0)(self.weight_logits))\n",
    "#         prob_list = []\n",
    "#         for i in range(self.num_components):\n",
    "#             if self.param_mode == \"full\":\n",
    "#                 scale_tril = fill_triangular(getattr(self, \"scale_{0}\".format(i)), self.dim)\n",
    "#                 scale_tril = matrix_diag_transform(scale_tril, F.softplus)\n",
    "#                 dist = MultivariateNormal(getattr(self, \"loc_{0}\".format(i)), scale_tril = scale_tril)\n",
    "#                 log_prob = dist.log_prob(input)\n",
    "#             elif self.param_mode == \"diag\":\n",
    "#                 dist = Normal(getattr(self, \"loc_{0}\".format(i)).unsqueeze(0), F.softplus(getattr(self, \"scale_{0}\".format(i))))\n",
    "#                 mu = getattr(self, \"loc_{0}\".format(i)).unsqueeze(0)\n",
    "#                 sigma = F.softplus(getattr(self, \"scale_{0}\".format(i)))\n",
    "#                 log_prob = (- (input - mu) ** 2 / 2 / sigma ** 2 - torch.log(sigma * np.sqrt(2 * np.pi))).sum(-1)\n",
    "#             else:\n",
    "#                 raise\n",
    "#             setattr(self, \"component_{0}\".format(i), dist)\n",
    "#             prob = torch.exp(log_prob)\n",
    "#             prob_list.append(prob)\n",
    "#         prob_list = torch.stack(prob_list, -1)\n",
    "#         prob = torch.matmul(prob_list, nn.Softmax(dim = 0)(self.weight_logits))\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        return torch.log(self.prob(input) + 1e-45)\n",
    "\n",
    "\n",
    "    def get_loss(self, X, y = None, **kwargs):\n",
    "        \"\"\"Optimize negative log-likelihood\"\"\"\n",
    "        neg_log_prob = - self.log_prob(X).mean() / np.log(2)\n",
    "        self.info_dict[\"loss\"] = to_np_array(neg_log_prob)\n",
    "        return neg_log_prob\n",
    "\n",
    "\n",
    "    def prepare_inspection(X, y, criterion, **kwargs):\n",
    "        return deepcopy(self.info_dict)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Mixture_Gaussian\"}\n",
    "        model_dict[\"num_components\"] = self.num_components\n",
    "        model_dict[\"dim\"] = self.dim\n",
    "        model_dict[\"param_mode\"] = self.param_mode\n",
    "        model_dict[\"weight_logits\"] = to_np_array(self.weight_logits)\n",
    "        model_dict[\"loc_list\"] = to_np_array(self.loc_list)\n",
    "        model_dict[\"scale_list\"] = to_np_array(self.scale_list)\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def get_param(self):\n",
    "        weights = to_np_array(nn.Softmax(dim = 0)(self.weight_logits))\n",
    "        loc_list = to_np_array(self.loc_list)\n",
    "        scale_list = to_np_array(self.scale_list)\n",
    "        print(\"weights: {0}\".format(weights))\n",
    "        print(\"loc:\")\n",
    "        pp.pprint(loc_list)\n",
    "        print(\"scale:\")\n",
    "        pp.pprint(scale_list)\n",
    "        return weights, loc_list, scale_list\n",
    "\n",
    "\n",
    "    def visualize(self, input):\n",
    "        import scipy\n",
    "        import matplotlib.pylab as plt\n",
    "        std = to_np_array(input.std())\n",
    "        X = np.arange(to_np_array(input.min()) - 0.2 * std, to_np_array(input.max()) + 0.2 * std, 0.1)\n",
    "        Y_dict = {}\n",
    "        weights = nn.Softmax(dim = 0)(self.weight_logits)\n",
    "        plt.figure(figsize=(10, 4), dpi=100).set_facecolor('white')\n",
    "        for i in range(self.num_components):\n",
    "            Y_dict[i] = weights[0].item() * scipy.stats.norm.pdf((X - self.loc_list[i].item()) / self.scale_list[i].item())\n",
    "            plt.plot(X, Y_dict[i])\n",
    "        Y = np.sum([item for item in Y_dict.values()], 0)\n",
    "        plt.plot(X, Y, 'k--')\n",
    "        plt.plot(input.data.numpy(), np.zeros(len(input)), 'k*')\n",
    "        plt.title('Density of {0}-component mixture model'.format(self.num_components))\n",
    "        plt.ylabel('probability density');\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weights\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = to_Variable([0], requires_grad = False).to(self.device)\n",
    "        return reg\n",
    "\n",
    "\n",
    "def load_model_dict_Mixture_Gaussian(model_dict, is_cuda = False):\n",
    "    model = Mixture_Gaussian(num_components = model_dict[\"num_components\"],\n",
    "                             dim = model_dict[\"dim\"],\n",
    "                             param_mode = model_dict[\"param_mode\"],\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "    model.initialize(model_dict = model_dict)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
