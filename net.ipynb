{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from pytorch_net.modules import get_Layer, load_layer_dict\n",
    "from pytorch_net.util import get_activation, get_criterion, get_optimizer, get_full_struct_param, plot_matrices, Early_Stopping, record_data, to_np_array, to_Variable, make_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, target):\n",
    "    \"\"\"Get accuracy from prediction and target\"\"\"\n",
    "    assert len(pred.shape) == len(target.shape) == 1\n",
    "    assert len(pred) == len(target)\n",
    "    pred, target = to_np_array(pred, target)\n",
    "    accuracy = ((pred == target).sum().astype(float) / len(pred))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def flatten(*tensors):\n",
    "    \"\"\"Flatten the tensor except the first dimension\"\"\"\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        new_tensors.append(tensor.view(tensor.size(0), -1))\n",
    "    if len(new_tensors) == 1:\n",
    "        new_tensors = new_tensors[0]\n",
    "    return new_tensors\n",
    "\n",
    "\n",
    "def fill_triangular(vec, dim, mode = \"lower\"):\n",
    "    \"\"\"Fill an lower or upper triangular matrices with given vectors\"\"\"\n",
    "    num_examples, size = vec.shape\n",
    "    assert size == dim * (dim + 1) // 2\n",
    "    matrix = torch.zeros(num_examples, dim, dim)\n",
    "    if vec.is_cuda:\n",
    "        matrix = matrix.cuda()\n",
    "    idx = (torch.tril(torch.ones(dim, dim)) == 1).unsqueeze(0)\n",
    "    idx = idx.repeat(num_examples,1,1)\n",
    "    if mode == \"lower\":\n",
    "        matrix[idx] = vec.contiguous().view(-1)\n",
    "    elif mode == \"upper\":\n",
    "        matrix[idx] = vec.contiguous().view(-1)\n",
    "    else:\n",
    "        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def matrix_diag_transform(matrix, fun):\n",
    "    \"\"\"Return the matrices whose diagonal elements have been executed by the function 'fun'.\"\"\"\n",
    "    num_examples = len(matrix)\n",
    "    idx = torch.eye(matrix.size(-1)).byte().unsqueeze(0)\n",
    "    idx = idx.repeat(num_examples, 1, 1)\n",
    "    new_matrix = matrix.clone()\n",
    "    new_matrix[idx] = fun(matrix.diagonal(dim1 = 1, dim2 = 2).contiguous().view(-1))\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def get_loss(model, data_loader = None, X = None, y = None, criterion = None):\n",
    "    \"\"\"Get loss using the whole data or data_loader\"\"\"\n",
    "    if data_loader is not None:\n",
    "        assert X is None and y is None\n",
    "        loss_list = []\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            loss_ele = model.get_loss(X_batch, y_batch, criterion = criterion)\n",
    "            loss_list.append(loss_ele)\n",
    "        loss = torch.stack(loss_list).mean()\n",
    "    else:\n",
    "        assert X is not None and y is not None\n",
    "        loss = model.get_loss(X, y, criterion = criterion)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def prepare_inspection(model, data_loader = None, X = None, y = None, **kwargs):\n",
    "    inspect_functions = kwargs[\"inspect_functions\"] if \"inspect_functions\" in kwargs else None\n",
    "    if data_loader is None:\n",
    "        assert X is not None and y is not None\n",
    "        all_dict_summary = model.prepare_inspection(X, y, **kwargs)\n",
    "        if inspect_functions is not None:\n",
    "            for inspect_function_key, inspect_function in inspect_functions.items():\n",
    "                all_dict_summary[inspect_function_key] = inspect_function(model, X, y, **kwargs)\n",
    "    else:\n",
    "        assert X is None and y is None\n",
    "        all_dict = {}\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            info_dict = model.prepare_inspection(X_batch, y_batch, **kwargs)\n",
    "            for key, item in info_dict.items():\n",
    "                if key not in all_dict:\n",
    "                    all_dict[key] = [item]\n",
    "                else:\n",
    "                    all_dict[key].append(item)\n",
    "            if inspect_functions is not None:\n",
    "                for inspect_function_key, inspect_function in inspect_functions.items():\n",
    "                    inspect_function_result = inspect_function(model, X_batch, y_batch, **kwargs)\n",
    "                    if inspect_function_key not in all_dict:\n",
    "                        all_dict[inspect_function_key] = [inspect_function_result]\n",
    "                    else:\n",
    "                        all_dict[inspect_function_key].append(inspect_function_result)\n",
    "        all_dict_summary = {}\n",
    "        for key, item in all_dict.items():\n",
    "            all_dict_summary[key] = np.mean(all_dict[key])\n",
    "    model.info_dict = all_dict_summary\n",
    "    return all_dict_summary\n",
    "    \n",
    "\n",
    "\n",
    "def train(model, X = None, y = None, train_loader = None, validation_data = None, validation_loader = None, criterion = nn.MSELoss(), inspect_interval = 10, isplot = False, **kwargs):\n",
    "    \"\"\"minimal version of training. \"model\" can be a single model or a ordered list of models\"\"\"\n",
    "    def get_regularization(model, **kwargs):\n",
    "        reg_dict = kwargs[\"reg_dict\"] if \"reg_dict\" in kwargs else {\"weight\": 0, \"bias\": 0}\n",
    "        reg = to_Variable([0], is_cuda = is_cuda)\n",
    "        for reg_type, reg_coeff in reg_dict.items():\n",
    "            reg = reg + model.get_regularization(source = reg_type, mode = \"L1\", **kwargs) * reg_coeff\n",
    "        return reg\n",
    "    if X is None and y is None:\n",
    "        assert train_loader is not None\n",
    "        is_cuda = train_loader.dataset.tensors[0].is_cuda\n",
    "    else:\n",
    "        is_cuda = X.is_cuda\n",
    "    epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 10000\n",
    "    lr = kwargs[\"lr\"] if \"lr\" in kwargs else 5e-3\n",
    "    optim_type = kwargs[\"optim_type\"] if \"optim_type\" in kwargs else \"adam\"\n",
    "    optim_kwargs = kwargs[\"optim_kwargs\"] if \"optim_kwargs\" in kwargs else {}\n",
    "    patience = kwargs[\"patience\"] if \"patience\" in kwargs else 20\n",
    "    record_keys = kwargs[\"record_keys\"] if \"record_keys\" in kwargs else [\"loss\"]\n",
    "    scheduler_type = kwargs[\"scheduler_type\"] if \"scheduler_type\" in kwargs else \"ReduceLROnPlateau\"\n",
    "    inspect_items = kwargs[\"inspect_items\"] if \"inspect_items\" in kwargs else None\n",
    "    inspect_functions = kwargs[\"inspect_functions\"] if \"inspect_functions\" in kwargs else None\n",
    "    if inspect_functions is not None:\n",
    "        for inspect_function_key in inspect_functions:\n",
    "            if inspect_function_key not in inspect_items:\n",
    "                inspect_items.append(inspect_function_key)\n",
    "    inspect_items_interval = kwargs[\"inspect_items_interval\"] if \"inspect_items_interval\" in kwargs else 1000\n",
    "    inspect_loss_precision = kwargs[\"inspect_loss_precision\"] if \"inspect_loss_precision\" in kwargs else 4\n",
    "    filename = kwargs[\"filename\"] if \"filename\" in kwargs else None\n",
    "    if filename is not None:\n",
    "        make_dir(filename)\n",
    "    save_interval = kwargs[\"save_interval\"] if \"save_interval\" in kwargs else None\n",
    "    logdir = kwargs[\"logdir\"] if \"logdir\" in kwargs else None\n",
    "    data_record = {key: [] for key in record_keys}\n",
    "    if patience is not None:\n",
    "        early_stopping_epsilon = kwargs[\"early_stopping_epsilon\"] if \"early_stopping_epsilon\" in kwargs else 0\n",
    "        early_stopping_monitor = kwargs[\"early_stopping_monitor\"] if \"early_stopping_monitor\" in kwargs else \"loss\"\n",
    "        early_stopping = Early_Stopping(patience = patience, epsilon = early_stopping_epsilon, mode = \"max\" if early_stopping_monitor in [\"accuracy\"] else \"min\")\n",
    "    if logdir is not None:\n",
    "        from pytorch_net.logger import Logger\n",
    "        batch_idx = 0\n",
    "        logger = Logger(logdir)\n",
    "    logimages = kwargs[\"logimages\"] if \"logimages\" in kwargs else None\n",
    "    \n",
    "    if validation_loader is not None:\n",
    "        assert validation_data is None\n",
    "        X_valid, y_valid = None, None\n",
    "    elif validation_data is not None:\n",
    "        X_valid, y_valid = validation_data\n",
    "    else:\n",
    "        X_valid, y_valid = X, y\n",
    "    \n",
    "    # Get original loss:\n",
    "    loss_original = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()\n",
    "    \n",
    "    if \"loss\" in record_keys:\n",
    "        record_data(data_record, [-1, get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()], [\"iter\", \"loss\"])\n",
    "    if \"param\" in record_keys:\n",
    "        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "    if \"param_grad\" in record_keys:\n",
    "        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "\n",
    "    # Setting up optimizer:\n",
    "    parameters = model.parameters()\n",
    "    num_params = len(list(model.parameters()))\n",
    "    if num_params == 0:\n",
    "        print(\"No parameters to optimize!\")\n",
    "        loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()\n",
    "        if \"loss\" in record_keys:\n",
    "            record_data(data_record, [0, get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()], [\"iter\", \"loss\"])\n",
    "        if \"param\" in record_keys:\n",
    "            record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "        if \"param_grad\" in record_keys:\n",
    "            record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "        return loss_original, loss_value, data_record\n",
    "    optimizer = get_optimizer(optim_type, lr, parameters, **optim_kwargs)\n",
    "    \n",
    "    # Set up learning rate scheduler:\n",
    "    if scheduler_type is not None:\n",
    "        if scheduler_type == \"ReduceLROnPlateau\":\n",
    "            scheduler_patience = kwargs[\"scheduler_patience\"] if \"scheduler_patience\" in kwargs else 40\n",
    "            scheduler_factor = kwargs[\"scheduler_factor\"] if \"scheduler_factor\" in kwargs else 0.1\n",
    "            scheduler_verbose = kwargs[\"scheduler_verbose\"] if \"scheduler_verbose\" in kwargs else False\n",
    "            scheduler = ReduceLROnPlateau(optimizer, factor = scheduler_factor, patience = scheduler_patience, verbose = scheduler_verbose)\n",
    "        elif scheduler_type == \"LambdaLR\":\n",
    "            scheduler_lr_lambda = kwargs[\"scheduler_lr_lambda\"] if \"scheduler_lr_lambda\" in kwargs else (lambda epoch: 1 / (1 + 0.01 * epoch))\n",
    "            scheduler = LambdaLR(optimizer, lr_lambda = scheduler_lr_lambda)\n",
    "        else:\n",
    "            raise\n",
    "        # First step:\n",
    "        if scheduler_type == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(loss_original)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    if inspect_items is not None:\n",
    "        print(\"{0}:\".format(-1), end = \"\")\n",
    "        print(\"\\tlr: {0:.3e}\\t loss:{1:.{2}f}\".format(optimizer.param_groups[0][\"lr\"], loss_original, inspect_loss_precision), end = \"\")\n",
    "        info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "        if len(info_dict) > 0:\n",
    "            for item in inspect_items:\n",
    "                if item in info_dict:\n",
    "                    print(\" \\t{0}: {1:.{2}f}\".format(item, info_dict[item], inspect_loss_precision), end = \"\")\n",
    "                    if item in record_keys and item != \"loss\":\n",
    "                        record_data(data_record, [to_np_array(info_dict[item])], [item])\n",
    "        print()\n",
    "            \n",
    "    if logdir is not None:\n",
    "        if logimages is not None:\n",
    "            for tag, image_fun in logimages[\"image_fun\"].items():\n",
    "                image = image_fun(model, logimages[\"X\"], logimages[\"y\"])\n",
    "                logger.log_images(tag, image, -1)\n",
    "\n",
    "    # Training:\n",
    "    to_stop = False\n",
    "    for i in range(epochs + 1):\n",
    "        model.train()\n",
    "        if X is not None and y is not None:\n",
    "            if optim_type != \"LBFGS\":\n",
    "                optimizer.zero_grad()\n",
    "                reg = get_regularization(model, **kwargs)\n",
    "                loss = model.get_loss(X, y, criterion = criterion, **kwargs) + reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                # \"LBFGS\" is a second-order optimization algorithm that requires a slightly different procedure:\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    reg = get_regularization(model, **kwargs)\n",
    "                    loss = model.get_loss(X, y, criterion = criterion, **kwargs) + reg\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                optimizer.step(closure)\n",
    "        else:\n",
    "            for _, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                if optim_type != \"LBFGS\":\n",
    "                    optimizer.zero_grad()\n",
    "                    reg = get_regularization(model, **kwargs)\n",
    "                    loss = model.get_loss(X_batch, y_batch, criterion = criterion, **kwargs) + reg\n",
    "                    loss.backward()\n",
    "                    if logdir is not None:\n",
    "                        batch_idx += 1\n",
    "                        if len(info_dict) > 0:\n",
    "                            for item in inspect_items:\n",
    "                                if item in info_dict:\n",
    "                                    logger.log_scalar(item, info_dict[item], batch_idx)\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        reg = get_regularization(model, **kwargs)\n",
    "                        loss = model.get_loss(X_batch, y_batch, criterion = criterion, **kwargs) + reg\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    if logdir is not None:\n",
    "                        batch_idx += 1\n",
    "                        if len(info_dict) > 0:\n",
    "                            for item in inspect_items:\n",
    "                                if item in info_dict:\n",
    "                                    logger.log_scalar(item, info_dict[item], batch_idx)\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "        if logdir is not None:\n",
    "            # Log values and gradients of the parameters (histogram summary)\n",
    "            for tag, value in model.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.log_histogram(tag, to_np_array(value), i)\n",
    "                logger.log_histogram(tag + '/grad', to_np_array(value.grad), i)\n",
    "            if logimages is not None:\n",
    "                for tag, image_fun in logimages[\"image_fun\"].items():\n",
    "                    image = image_fun(model, logimages[\"X\"], logimages[\"y\"])\n",
    "                    logger.log_images(tag, image, i)\n",
    "\n",
    "        if i % inspect_interval == 0:\n",
    "            model.eval()\n",
    "            loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()\n",
    "            if scheduler_type is not None:\n",
    "                if scheduler_type == \"ReduceLROnPlateau\":\n",
    "                    scheduler.step(loss_value)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "            if patience is not None:\n",
    "                if early_stopping_monitor == \"loss\":\n",
    "                    to_stop = early_stopping.monitor(loss_value)\n",
    "                else:\n",
    "                    info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "                    to_stop = early_stopping.monitor(info_dict[early_stopping_monitor])\n",
    "            if inspect_items is not None:\n",
    "                if i % inspect_items_interval == 0:\n",
    "                    print(\"{0}:\".format(i), end = \"\")\n",
    "                    print(\"\\tlr: {0:.3e}\\tloss: {1:.{2}f}\".format(optimizer.param_groups[0][\"lr\"], loss_value, inspect_loss_precision), end = \"\")\n",
    "                    info_dict = prepare_inspection(model, validation_loader, X_valid, y_valid, **kwargs)\n",
    "                    if len(info_dict) > 0:\n",
    "                        for item in inspect_items:\n",
    "                            if item in info_dict:\n",
    "                                print(\" \\t{0}: {1:.{2}f}\".format(item, info_dict[item], inspect_loss_precision), end = \"\")\n",
    "                                if item in record_keys and item != \"loss\":\n",
    "                                    record_data(data_record, [to_np_array(info_dict[item])], [item])\n",
    "                    if \"loss\" in record_keys:\n",
    "                        record_data(data_record, [i, loss_value], [\"iter\", \"loss\"])\n",
    "                    if \"param\" in record_keys:\n",
    "                        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\")], [\"param\"])\n",
    "                    if \"param_grad\" in record_keys:\n",
    "                        record_data(data_record, [model.get_weights_bias(W_source = \"core\", b_source = \"core\", is_grad = True)], [\"param_grad\"])\n",
    "                    print()\n",
    "        if filename is not None and save_interval is not None:\n",
    "            if i % save_interval == 0:\n",
    "                pickle.dump(model.model_dict, open(filename[:-2] + \"_{0}\".format(i) + \".p\", \"wb\"))\n",
    "        if to_stop:\n",
    "            break\n",
    "\n",
    "    loss_value = get_loss(model, validation_loader, X_valid, y_valid, criterion = criterion).item()\n",
    "    if isplot:\n",
    "        import matplotlib.pylab as plt\n",
    "        for key in data_record:\n",
    "            if key != \"iter\":\n",
    "                if key in [\"accuracy\"]:\n",
    "                    plt.figure(figsize = (8,6))\n",
    "                    plt.plot(data_record[\"iter\"], data_record[key])\n",
    "                    plt.xlabel(\"epoch\")\n",
    "                    plt.ylabel(key)\n",
    "                    plt.title(key)\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    plt.figure(figsize = (8,6))\n",
    "                    plt.semilogy(data_record[\"iter\"], data_record[key])\n",
    "                    plt.xlabel(\"epoch\")\n",
    "                    plt.ylabel(key)\n",
    "                    plt.title(key)\n",
    "                    plt.show()\n",
    "    return loss_original, loss_value, data_record\n",
    "\n",
    "\n",
    "def load_model_dict_net(model_dict, is_cuda = False):\n",
    "    net_type = model_dict[\"type\"]\n",
    "    if net_type == \"MLP\":\n",
    "        return MLP(input_size = model_dict[\"input_size\"],\n",
    "                   struct_param = model_dict[\"struct_param\"],\n",
    "                   W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                   b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                   settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                   is_cuda = is_cuda,\n",
    "                  )\n",
    "    elif net_type == \"Multi_MLP\":\n",
    "        return Multi_MLP(input_size = model_dict[\"input_size\"],\n",
    "                   struct_param = model_dict[\"struct_param\"],\n",
    "                   W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                   b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                   settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                   is_cuda = is_cuda,\n",
    "                  )\n",
    "    elif net_type == \"ConvNet\":\n",
    "        return ConvNet(input_channels = model_dict[\"input_channels\"],\n",
    "                       struct_param = model_dict[\"struct_param\"],\n",
    "                       W_init_list = model_dict[\"weights\"] if \"weights\" in model_dict else None,\n",
    "                       b_init_list = model_dict[\"bias\"] if \"bias\" in model_dict else None,\n",
    "                       settings = model_dict[\"settings\"] if \"settings\" in model_dict else {},\n",
    "                       return_indices = model_dict[\"return_indices\"] if \"return_indices\" in model_dict else False,\n",
    "                       is_cuda = is_cuda,\n",
    "                      )\n",
    "    elif net_type == \"Conv_Autoencoder\":\n",
    "        model = Conv_Autoencoder(input_channels_encoder = model_dict[\"input_channels_encoder\"],\n",
    "                                 input_channels_decoder = model_dict[\"input_channels_decoder\"],\n",
    "                                 struct_param_encoder = model_dict[\"struct_param_encoder\"],\n",
    "                                 struct_param_decoder = model_dict[\"struct_param_decoder\"],\n",
    "                                 settings = model_dict[\"settings\"],\n",
    "                                 is_cuda = is_cuda,\n",
    "                                )\n",
    "        if \"encoder\" in model_dict:\n",
    "            model.encoder.load_model_dict(model_dict[\"encoder\"])\n",
    "        if \"decoder\" in model_dict:\n",
    "            model.decoder.load_model_dict(model_dict[\"decoder\"])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"net_type {0} not recognized!\".format(net_type))\n",
    "        \n",
    "\n",
    "def load_model_dict(model_dict, is_cuda = False):\n",
    "    net_type = model_dict[\"type\"]\n",
    "    if net_type not in [\"Model_Ensemble\", \"LSTM\"]:\n",
    "        return load_model_dict_net(model_dict, is_cuda = is_cuda)\n",
    "    elif net_type == \"Model_Ensemble\":\n",
    "        if model_dict[\"model_type\"] == \"MLP\":\n",
    "            model_ensemble = Model_Ensemble(\n",
    "                num_models = model_dict[\"num_models\"],\n",
    "                input_size = model_dict[\"input_size\"],\n",
    "                model_type = model_dict[\"model_type\"],\n",
    "                output_size = model_dict[\"output_size\"],\n",
    "                is_cuda = is_cuda,\n",
    "                # Here we just create some placeholder network. The model will be overwritten in the next steps:\n",
    "                struct_param = [[1, \"Simple_Layer\", {}]],\n",
    "            )\n",
    "        elif model_dict[\"model_type\"] == \"LSTM\":\n",
    "            model_ensemble = Model_Ensemble(\n",
    "                num_models = model_dict[\"num_models\"],\n",
    "                input_size = model_dict[\"input_size\"],\n",
    "                model_type = model_dict[\"model_type\"],\n",
    "                output_size = model_dict[\"output_size\"],\n",
    "                is_cuda = is_cuda,\n",
    "                # Here we just create some placeholder network. The model will be overwritten in the next steps:\n",
    "                hidden_size = 3,\n",
    "                output_struct_param = [[1, \"Simple_Layer\", {}]],\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "        for k in range(model_ensemble.num_models):\n",
    "            setattr(model_ensemble, \"model_{0}\".format(k), load_model_dict(model_dict[\"model_{0}\".format(k)], is_cuda = is_cuda))\n",
    "        return model_ensemble\n",
    "    elif net_type == \"Model_with_Uncertainty\":\n",
    "        return Model_with_Uncertainty(model_pred = load_model_dict(model_dict[\"model_pred\"], is_cuda = is_cuda),\n",
    "                                      model_logstd = load_model_dict(model_dict[\"model_logstd\"], is_cuda = is_cuda))\n",
    "    else:\n",
    "        raise Exception(\"net_type {0} not recognized!\".format(net_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_Ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Ensemble(nn.Module):\n",
    "    \"\"\"Model_Ensemble is a collection of models with the same architecture \n",
    "       but independent parameters\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models,\n",
    "        input_size,\n",
    "        model_type,\n",
    "        is_cuda = False,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super(Model_Ensemble, self).__init__()\n",
    "        self.num_models = num_models\n",
    "        self.input_size = input_size\n",
    "        self.is_cuda = is_cuda\n",
    "        for i in range(self.num_models):\n",
    "            if model_type == \"MLP\":\n",
    "                model = MLP(input_size = self.input_size, is_cuda = is_cuda, **kwargs)\n",
    "            elif model_type == \"LSTM\":\n",
    "                model = LSTM(input_size = self.input_size, is_cuda = is_cuda, **kwargs)\n",
    "            else:\n",
    "                raise Exception(\"Net_type {0} not recognized!\".format(net_type))\n",
    "            setattr(self, \"model_{0}\".format(i), model)\n",
    "\n",
    "\n",
    "    def get_all_models(self):\n",
    "        return [getattr(self, \"model_{0}\".format(i)) for i in range(self.num_models)]\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_list = []\n",
    "        for i in range(self.num_models):\n",
    "            output = getattr(self, \"model_{0}\".format(i))(input)\n",
    "            if output.size(-1) == 1:\n",
    "                output = output.squeeze(1)\n",
    "            output_list.append(output)\n",
    "        return torch.stack(output_list, -1)\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(self.num_models):\n",
    "            reg = reg + getattr(self, \"model_{0}\".format(k)).get_regularization(\n",
    "                source = source, mode = mode, **kwargs)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def remove_models(self, model_ids):\n",
    "        if not isinstance(model_ids, list):\n",
    "            model_ids = [model_ids]\n",
    "        model_list = []\n",
    "        k = 0\n",
    "        for i in range(self.num_models):\n",
    "            if i not in model_ids:\n",
    "                if k != i:\n",
    "                    setattr(self, \"model_{0}\".format(k), getattr(self, \"model_{0}\".format(i)))\n",
    "                k += 1\n",
    "        num_models_new = k\n",
    "        for i in range(num_models_new, self.num_models):\n",
    "            delattr(self, \"model_{0}\".format(i))\n",
    "        self.num_models = num_models_new\n",
    "\n",
    "\n",
    "    def add_models(self, models):\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "        for i, model in enumerate(models):\n",
    "            setattr(self, \"model_{0}\".format(i + self.num_models), model)\n",
    "        self.num_models += len(models)\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, verbose = False, isplot = False):\n",
    "        W_list_dict = {}\n",
    "        b_list_dict = {}\n",
    "        for i in range(self.num_models):\n",
    "            if verbose:\n",
    "                print(\"\\nmodel {0}:\".format(i))\n",
    "            W_list_dict[i], b_list_dict[i] = getattr(self, \"model_{0}\".format(i)).get_weights_bias(\n",
    "                W_source = W_source, b_source = b_source, verbose = verbose, isplot = isplot)\n",
    "        return W_list_dict, b_list_dict\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_models):\n",
    "            getattr(self, \"model_{0}\".format(k)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "    \n",
    "    \n",
    "    def set_trainable(self, is_trainable):\n",
    "        for i in range(self.num_models):\n",
    "            getattr(self, \"model_{0}\".format(i)).set_trainable(is_trainable)\n",
    "    \n",
    "\n",
    "class Model_with_uncertainty(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_pred,\n",
    "        model_logstd,\n",
    "        ):\n",
    "        super(Model_with_uncertainty, self).__init__()\n",
    "        self.model_pred = model_pred\n",
    "        self.model_logstd = model_logstd\n",
    "        \n",
    "    def forward(self, input, noise_amp = None, **kwargs):\n",
    "        return self.model_pred(input, noise_amp = noise_amp, **kwargs), self.model_logstd(input, **kwargs)\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, noise_amp = None, **kwargs):\n",
    "        pred, log_std = self(input, noise_amp = noise_amp, **kwargs)\n",
    "        return criterion(pred = pred, target = target, log_std = log_std)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        return self.model_pred.get_regularization(source = source, mode = mode, **kwargs) +                 self.model_logstd.get_regularization(source = source, mode = mode, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {}\n",
    "        model_dict[\"type\"] = \"Model_with_Uncertainty\"\n",
    "        model_dict[\"model_pred\"] = self.model_pred.model_dict\n",
    "        model_dict[\"model_logstd\"] = self.model_logstd.model_dict\n",
    "        return model_dict\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        self.model_pred.set_cuda(is_cuda)\n",
    "        self.model_logstd.set_cuda(is_cuda)\n",
    "        \n",
    "    def set_trainable(self, is_trainable):\n",
    "        self.model_pred.set_trainable(is_trainable)\n",
    "        self.model_logstd.set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi_MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = None,          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Multi_MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = deepcopy(settings)\n",
    "        self.num_blocks = len(struct_param)\n",
    "        self.is_cuda = is_cuda\n",
    "        \n",
    "        for i, struct_param_ele in enumerate(struct_param):\n",
    "            input_size_block = input_size if i == 0 else struct_param[i - 1][-1][0]\n",
    "            setattr(self, \"block_{0}\".format(i), MLP(input_size = input_size_block,\n",
    "                                                     struct_param = struct_param_ele,\n",
    "                                                     W_init_list = W_init_list[i] if W_init_list is not None else None,\n",
    "                                                     b_init_list = b_init_list[i] if b_init_list is not None else None,\n",
    "                                                     settings = self.settings[i] if self.settings is not None else {},\n",
    "                                                     is_cuda = self.is_cuda,\n",
    "                                                    ))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for i in range(self.num_blocks):\n",
    "            output = getattr(self, \"block_{0}\".format(i))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for i in range(self.num_blocks):\n",
    "            reg = reg + getattr(self, \"block_{0}\".format(i)).get_regularization(mode = mode, source = source)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [getattr(self, \"block_{0}\".format(i)).struct_param for i in range(self.num_blocks)]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Multi_MLP\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"struct_param\"] = self.struct_param\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"settings\"] = deepcopy(self.settings)\n",
    "        model_dict[\"net_type\"] = \"Multi_MLP\"\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_Multi_MLP(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = \"core\", b_source = \"core\"):\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        for i in range(self.num_blocks):\n",
    "            W, b = getattr(self, \"block_{0}\".format(i)).get_weights_bias(W_source = W_source, b_source = b_source)\n",
    "            W_list.append(W)\n",
    "            b_list.append(b)\n",
    "        return deepcopy(W_list), deepcopy(b_list)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        for i in range(self.num_blocks):\n",
    "            getattr(self, \"block_{0}\".format(i)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for i in range(self.num_blocks):\n",
    "            getattr(self, \"block_{0}\".format(i)).set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = {},          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = deepcopy(settings)\n",
    "        self.is_cuda = is_cuda\n",
    "        self.info_dict = {}\n",
    "        \n",
    "        self.init_layers(deepcopy(struct_param))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [getattr(self, \"layer_{0}\".format(i)).struct_param for i in range(self.num_layers)]\n",
    "\n",
    "\n",
    "    def init_layers(self, struct_param):\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        for k, layer_struct_param in enumerate(struct_param):\n",
    "            if res_forward:\n",
    "                num_neurons_prev = struct_param[k - 1][0] + self.input_size if k > 0 else self.input_size\n",
    "            else:\n",
    "                num_neurons_prev = struct_param[k - 1][0] if k > 0 else self.input_size\n",
    "            num_neurons = layer_struct_param[0]\n",
    "            W_init = self.W_init_list[k] if self.W_init_list is not None else None\n",
    "            b_init = self.b_init_list[k] if self.b_init_list is not None else None\n",
    "\n",
    "            # Get settings for the current layer:\n",
    "            layer_settings = deepcopy(self.settings) if bool(self.settings) else {}\n",
    "            layer_settings.update(layer_struct_param[2])            \n",
    "\n",
    "            # Construct layer:\n",
    "            layer = get_Layer(layer_type = layer_struct_param[1],\n",
    "                              input_size = num_neurons_prev,\n",
    "                              output_size = num_neurons,\n",
    "                              W_init = W_init,\n",
    "                              b_init = b_init,\n",
    "                              settings = layer_settings,\n",
    "                              is_cuda = self.is_cuda,\n",
    "                             )\n",
    "            setattr(self, \"layer_{0}\".format(k), layer)\n",
    "\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        output = input\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        is_res_block = self.settings[\"is_res_block\"] if \"is_res_block\" in self.settings else False\n",
    "        for k in range(len(self.struct_param)):\n",
    "            if res_forward and k > 0:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(torch.cat([output, input], -1))\n",
    "            else:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "        if is_res_block:\n",
    "            output = output + input\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(len(self.struct_param)):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            reg = reg + layer.get_regularization(mode = mode, source = source)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def reset_layer(self, layer_id, layer):\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "\n",
    "\n",
    "    def insert_layer(self, layer_id, layer):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if next_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert next_layer.input_size == layer.output_size, \"The inserted layer's output_size {0} must be compatible with next layer_{1}'s input_size {2}!\"\\\n",
    "                    .format(layer.output_size, layer_id + 1, next_layer.input_size)\n",
    "        for i in range(self.num_layers - 1, layer_id - 1, -1):\n",
    "            setattr(self, \"layer_{0}\".format(i + 1), getattr(self, \"layer_{0}\".format(i)))\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "        self.num_layers += 1\n",
    "    \n",
    "    \n",
    "    def remove_layer(self, layer_id):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            num_neurons_prev = self.struct_param[layer_id - 1][0] if layer_id > 0 else self.input_size\n",
    "            replaced_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if replaced_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert replaced_layer.input_size == num_neurons_prev, \\\n",
    "                    \"After deleting layer_{0}, the replaced layer's input_size {1} must be compatible with previous layer's output neurons {2}!\"\\\n",
    "                        .format(layer_id, replaced_layer.input_size, num_neurons_prev)\n",
    "        for i in range(layer_id, self.num_layers - 1):\n",
    "            setattr(self, \"layer_{0}\".format(i), getattr(self, \"layer_{0}\".format(i + 1)))\n",
    "        self.num_layers -= 1\n",
    "\n",
    "\n",
    "    def prune_neurons(self, layer_id, neuron_ids):\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.prune_output_neurons(neuron_ids)\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.prune_input_neurons(neuron_ids)\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "\n",
    "\n",
    "    def add_neurons(self, layer_id, num_neurons, mode = (\"imitation\", \"zeros\")):\n",
    "        if not isinstance(mode, list) and not isinstance(mode, tuple):\n",
    "            mode = (mode, mode)\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.add_output_neurons(num_neurons, mode = mode[0])\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.add_input_neurons(num_neurons, mode = mode[1])\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "\n",
    "\n",
    "    def inspect_operation(self, input, operation_between):\n",
    "        res_forward = self.settings[\"res_forward\"] if \"res_forward\" in self.settings else False\n",
    "        output = input\n",
    "        for k in range(*operation_between):\n",
    "            output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "            if res_forward and k > 0:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(torch.cat([output, input], -1))\n",
    "            else:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, layer_ids = None, isplot = False, raise_error = True):\n",
    "        layer_ids = range(len(self.struct_param)) if layer_ids is None else layer_ids\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        if W_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if W_source == \"core\":\n",
    "                        try:\n",
    "                            W, _ = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            W = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"W_source '{0}' not recognized!\".format(W_source))\n",
    "                    W_list.append(W)\n",
    "        \n",
    "        if b_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if b_source == \"core\":\n",
    "                        try:\n",
    "                            _, b = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            b = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"b_source '{0}' not recognized!\".format(b_source))\n",
    "                b_list.append(b)\n",
    "                \n",
    "        if isplot:\n",
    "            if W_source is not None:\n",
    "                print(\"weight {0}:\".format(W_source))\n",
    "                plot_matrices(W_list)\n",
    "            if b_source is not None:\n",
    "                print(\"bias {0}:\".format(b_source))\n",
    "                plot_matrices(b_list)\n",
    "        return W_list, b_list\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"MLP\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"struct_param\"] = get_full_struct_param(self.struct_param, self.settings)\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"settings\"] = deepcopy(self.settings)\n",
    "        model_dict[\"net_type\"] = \"MLP\"\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_layers):\n",
    "            getattr(self, \"layer_{0}\".format(k)).set_cuda(is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for k in range(self.num_layers):\n",
    "            getattr(self, \"layer_{0}\".format(k)).set_trainable(is_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCellBase(nn.Module):\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if 'bias' in self.__dict__ and self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
    "            s += ', nonlinearity={nonlinearity}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def check_forward_input(self, input):\n",
    "        if input.size(1) != self.input_size:\n",
    "            raise RuntimeError(\n",
    "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
    "                    input.size(1), self.input_size))\n",
    "\n",
    "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
    "        if input.size(0) != hx.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
    "                    input.size(0), hidden_label, hx.size(0)))\n",
    "\n",
    "        if hx.size(1) != self.hidden_size:\n",
    "            raise RuntimeError(\n",
    "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
    "                    hidden_label, hx.size(1), self.hidden_size))\n",
    "\n",
    "\n",
    "class LSTM(RNNCellBase):\n",
    "    \"\"\"a LSTM class\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_struct_param,\n",
    "        output_settings = {},\n",
    "        bias = True,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.W_ih = nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n",
    "        self.W_hh = nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n",
    "        self.output_net = MLP(input_size = self.hidden_size, struct_param = output_struct_param, settings = output_settings, is_cuda = is_cuda)\n",
    "        if bias:\n",
    "            self.b_ih = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "            self.b_hh = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('b_ih', None)\n",
    "            self.register_parameter('b_hh', None)\n",
    "        self.reset_parameters()\n",
    "        self.is_cuda = is_cuda\n",
    "        self.device = torch.device(\"cuda\" if self.is_cuda else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward_one_step(self, input, hx):\n",
    "        self.check_forward_input(input)\n",
    "        self.check_forward_hidden(input, hx[0], '[0]')\n",
    "        self.check_forward_hidden(input, hx[1], '[1]')\n",
    "        return self._backend.LSTMCell(\n",
    "            input, hx,\n",
    "            self.W_ih, self.W_hh,\n",
    "            self.b_ih, self.b_hh,\n",
    "        )\n",
    "    \n",
    "    def forward(self, input, hx = None):\n",
    "        if hx is None:\n",
    "            hx = [torch.randn(input.size(0), self.hidden_size).to(self.device),\n",
    "                  torch.randn(input.size(0), self.hidden_size).to(self.device),\n",
    "                 ]\n",
    "        hhx, ccx = hx\n",
    "        for i in range(input.size(1)):\n",
    "            hhx, ccx = self.forward_one_step(input[:, i], (hhx, ccx))\n",
    "        output = self.output_net(hhx)\n",
    "        return output\n",
    "\n",
    "    def get_regularization(self, source, mode = \"L1\", **kwargs):\n",
    "        if not isinstance(source, list):\n",
    "            source = [source]\n",
    "        reg = self.output_net.get_regularization(source = source, mode = mode)\n",
    "        for source_ele in source:\n",
    "            if source_ele == \"weight\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.W_ih.abs().sum() + self.W_hh.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.W_ih ** 2).sum() + (self.W_hh ** 2).sum()\n",
    "                else:\n",
    "                    raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "            elif source_ele == \"bias\":\n",
    "                if self.bias:\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + self.b_ih.abs().sum() + self.b_hh.abs().sum()\n",
    "                    elif mode == \"L2\":\n",
    "                        reg = reg + (self.b_ih ** 2).sum() + (self.b_hh ** 2).sum()\n",
    "                    else:\n",
    "                        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "            else:\n",
    "                raise Exception(\"source {0} not recognized!\".format(source_ele))\n",
    "        return reg\n",
    "    \n",
    "    def get_weights_bias(self, W_source = None, b_source = None, verbose = False, isplot = False):\n",
    "        W_dict = OrderedDict()\n",
    "        b_dict = OrderedDict()\n",
    "        W_o, b_o = self.output_net.get_weights_bias(W_source = W_source, b_source = b_source)\n",
    "        if W_source == \"core\":\n",
    "            W_dict[\"W_ih\"] = self.W_ih.cpu().detach().numpy()\n",
    "            W_dict[\"W_hh\"] = self.W_hh.cpu().detach().numpy()\n",
    "            W_dict[\"W_o\"] = W_o\n",
    "            if isplot:\n",
    "                print(\"W_ih, W_hh:\")\n",
    "                plot_matrices([W_dict[\"W_ih\"], W_dict[\"W_hh\"]])\n",
    "                print(\"W_o:\")\n",
    "                plot_matrices(W_o)\n",
    "        if self.bias and b_source == \"core\":\n",
    "            b_dict[\"b_ih\"] = self.b_ih.cpu().detach().numpy()\n",
    "            b_dict[\"b_hh\"] = self.b_hh.cpu().detach().numpy()\n",
    "            b_dict[\"b_o\"] = b_o\n",
    "            if isplot:\n",
    "                print(\"b_ih, b_hh:\")\n",
    "                plot_matrices([b_dict[\"b_ih\"], b_dict[\"b_hh\"]])\n",
    "                print(\"b_o:\")\n",
    "                plot_matrices(b_o)\n",
    "        return W_dict, b_dict\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, hx = None, **kwargs):\n",
    "        y_pred = self(input, hx = hx)\n",
    "        return criterion(y_pred, target)\n",
    "    \n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        struct_param,\n",
    "        W_init_list = None,\n",
    "        b_init_list = None,\n",
    "        settings = {},\n",
    "        return_indices = False,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.struct_param = struct_param\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = settings\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.info_dict = {}\n",
    "        self.is_cuda = is_cuda\n",
    "        self.param_available = [\"Conv2d\", \"ConvTranspose2d\", \"BatchNorm2d\", \"Simple_Layer\"]\n",
    "        self.return_indices = return_indices\n",
    "        for i in range(len(self.struct_param)):\n",
    "            if i > 0:\n",
    "                k = 1\n",
    "                while self.struct_param[i - k][0] is None:\n",
    "                    k += 1\n",
    "                num_channels_prev = self.struct_param[i - k][0]\n",
    "            else:\n",
    "                num_channels_prev = input_channels\n",
    "            num_channels = self.struct_param[i][0]\n",
    "            layer_type = self.struct_param[i][1]\n",
    "            layer_settings = self.struct_param[i][2]\n",
    "            if \"layer_input_size\" in layer_settings and isinstance(layer_settings[\"layer_input_size\"], tuple):\n",
    "                num_channels_prev = layer_settings[\"layer_input_size\"][0]\n",
    "            if layer_type == \"Conv2d\":\n",
    "                layer = nn.Conv2d(num_channels_prev, \n",
    "                                  num_channels,\n",
    "                                  kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                  stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else 1,\n",
    "                                  padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                  dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                 )\n",
    "            elif layer_type == \"ConvTranspose2d\":\n",
    "                layer = nn.ConvTranspose2d(num_channels_prev,\n",
    "                                           num_channels,\n",
    "                                           kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                           stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else 1,\n",
    "                                           padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                           dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                          )\n",
    "            elif layer_type == \"Simple_Layer\":\n",
    "                layer = get_Layer(layer_type = layer_type,\n",
    "                                  input_size = layer_settings[\"layer_input_size\"],\n",
    "                                  output_size = num_channels,\n",
    "                                  W_init = W_init_list[i] if self.W_init_list is not None and self.W_init_list[i] is not None else None,\n",
    "                                  b_init = b_init_list[i] if self.b_init_list is not None and self.b_init_list[i] is not None else None,\n",
    "                                  settings = layer_settings,\n",
    "                                  is_cuda = self.is_cuda,\n",
    "                                 )\n",
    "            elif layer_type == \"MaxPool2d\":\n",
    "                layer = nn.MaxPool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                     stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                     padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                     return_indices = layer_settings[\"return_indices\"] if \"return_indices\" in layer_settings else False,\n",
    "                                    )\n",
    "            elif layer_type == \"MaxUnpool2d\":\n",
    "                layer = nn.MaxUnpool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                       stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                       padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                      )\n",
    "            elif layer_type == \"Upsample\":\n",
    "                layer = nn.Upsample(scale_factor = layer_settings[\"scale_factor\"],\n",
    "                                    mode = layer_settings[\"mode\"] if \"mode\" in layer_settings else \"nearest\",\n",
    "                                   )\n",
    "            elif layer_type == \"BatchNorm2d\":\n",
    "                layer = nn.BatchNorm2d(num_features = num_channels)\n",
    "            elif layer_type == \"Dropout2d\":\n",
    "                layer = nn.Dropout2d(p = 0.5)\n",
    "            elif layer_type == \"Flatten\":\n",
    "                layer = Flatten()\n",
    "            else:\n",
    "                raise Exception(\"layer_type {0} not recognized!\".format(layer_type))\n",
    "            \n",
    "            # Initialize using provided initial values:\n",
    "            if self.W_init_list is not None and self.W_init_list[i] is not None and layer_type not in [\"Simple_Layer\"]:\n",
    "                layer.weight.data = torch.FloatTensor(self.W_init_list[i])\n",
    "                layer.bias.data = torch.FloatTensor(self.b_init_list[i])\n",
    "            \n",
    "            setattr(self, \"layer_{0}\".format(i), layer)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    def forward(self, input, indices_list = None, **kwargs):\n",
    "        return self.inspect_operation(input, operation_between = (0, self.num_layers), indices_list = indices_list)\n",
    "    \n",
    "    \n",
    "    def inspect_operation(self, input, operation_between, indices_list = None):\n",
    "        output = input\n",
    "        if indices_list is None:\n",
    "            indices_list = []\n",
    "        start_layer, end_layer = operation_between\n",
    "        if end_layer < 0:\n",
    "            end_layer += self.num_layers\n",
    "        for i in range(start_layer, end_layer):\n",
    "            if \"layer_input_size\" in self.struct_param[i][2]:\n",
    "                output_size_last = output.shape[0]\n",
    "                layer_input_size = self.struct_param[i][2][\"layer_input_size\"]\n",
    "                if not isinstance(layer_input_size, tuple):\n",
    "                    layer_input_size = (layer_input_size,)\n",
    "                output = output.view(-1, *layer_input_size)\n",
    "                assert output.shape[0] == output_size_last, \"output_size reshaped to different length. Check shape!\"\n",
    "            if \"Unpool\" in self.struct_param[i][1]:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output, indices_list.pop(-1))\n",
    "            else:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output)\n",
    "            if isinstance(output_tentative, tuple):\n",
    "                output, indices = output_tentative\n",
    "                indices_list.append(indices)\n",
    "            else:\n",
    "                output = output_tentative\n",
    "            if \"activation\" in self.struct_param[i][2]:\n",
    "                activation = self.struct_param[i][2][\"activation\"]\n",
    "            else:\n",
    "                if \"activation\" in self.settings:\n",
    "                    activation = self.settings[\"activation\"]\n",
    "                else:\n",
    "                    activation = \"linear\"\n",
    "                if \"Pool\" in self.struct_param[i][1] or \"Unpool\" in self.struct_param[i][1] or \"Upsample\" in self.struct_param[i][1]:\n",
    "                    activation = \"linear\"\n",
    "            output = get_activation(activation)(output)\n",
    "        if self.return_indices:\n",
    "            return output, indices_list\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        y_pred = self(input, **kwargs)\n",
    "        if self.return_indices:\n",
    "            y_pred = y_pred[0]\n",
    "        return criterion(y_pred, target)\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        if not isinstance(source, list):\n",
    "            source = [source]\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] not in self.param_available:\n",
    "                continue\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            for source_ele in source:\n",
    "                if source_ele == \"weight\":\n",
    "                    if self.struct_param[k][1] not in [\"Simple_Layer\"]:\n",
    "                        item = layer.weight\n",
    "                    else:\n",
    "                        item = layer.W_core\n",
    "                elif source_ele == \"bias\":\n",
    "                    if self.struct_param[k][1] not in [\"Simple_Layer\"]:\n",
    "                        item = layer.bias\n",
    "                    else:\n",
    "                        item = layer.b_core\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + item.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (item ** 2).sum()\n",
    "                else:\n",
    "                    raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = \"core\", b_source = \"core\"):\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(to_np_array(layer.W_core))\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(to_np_array(layer.b_core))\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(to_np_array(layer.weight))\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(to_np_array(layer.bias))\n",
    "            else:\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(None)\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(None)\n",
    "        return W_list, b_list\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"ConvNet\"}\n",
    "        model_dict[\"net_type\"] = \"ConvNet\"\n",
    "        model_dict[\"input_channels\"] = self.input_channels\n",
    "        model_dict[\"struct_param\"] = self.struct_param\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"return_indices\"] = self.return_indices\n",
    "        return model_dict\n",
    "    \n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "\n",
    "    def prepare_inspection(self, X, y, **kwargs):\n",
    "        pred_prob = self(X)\n",
    "        if self.return_indices:\n",
    "            pred_prob = pred_prob[0]\n",
    "        pred = pred_prob.max(1)[1]\n",
    "        self.info_dict[\"accuracy\"] = get_accuracy(pred, y)\n",
    "        return deepcopy(self.info_dict)\n",
    "    \n",
    "    \n",
    "    def set_cuda(self, is_cuda):\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                getattr(self, \"layer_{0}\".format(k)).set_cuda(is_cuda)\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                if is_cuda is True:\n",
    "                    getattr(self, \"layer_{0}\".format(k)).cuda()\n",
    "                else:\n",
    "                    getattr(self, \"layer_{0}\".format(k)).cpu()\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        for k in range(self.num_layers):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            if self.struct_param[k][1] == \"Simple_Layer\":\n",
    "                layer.set_trainable(is_trainable)\n",
    "            elif self.struct_param[k][1] in self.param_available:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = is_trainable\n",
    "\n",
    "\n",
    "class Conv_Autoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels_encoder,\n",
    "        input_channels_decoder,\n",
    "        struct_param_encoder,\n",
    "        struct_param_decoder,\n",
    "        latent_size = (1,2),\n",
    "        share_model_among_steps = False,\n",
    "        settings = {},\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Conv_Autoencoder, self).__init__()\n",
    "        self.input_channels_encoder = input_channels_encoder\n",
    "        self.input_channels_decoder = input_channels_decoder\n",
    "        self.struct_param_encoder = struct_param_encoder\n",
    "        self.struct_param_decoder = struct_param_decoder\n",
    "        self.share_model_among_steps = share_model_among_steps\n",
    "        self.settings = settings\n",
    "        self.encoder = ConvNet(input_channels = input_channels_encoder, struct_param = struct_param_encoder, settings = settings, is_cuda = is_cuda)\n",
    "        self.decoder = ConvNet(input_channels = input_channels_decoder, struct_param = struct_param_decoder, settings = settings, is_cuda = is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "    \n",
    "    def encode(self, input):\n",
    "        if self.share_model_among_steps:\n",
    "            latent = []\n",
    "            for i in range(input.shape[1]):\n",
    "                latent_step = self.encoder(input[:, i:i+1])\n",
    "                latent.append(latent_step)\n",
    "            return torch.cat(latent, 1)\n",
    "        else:\n",
    "            return self.encoder(input)\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        if self.share_model_among_steps:\n",
    "            latent_size = self.struct_param_encoder[-1][0]\n",
    "            latent = latent.view(latent.size(0), -1, latent_size)\n",
    "            output = []\n",
    "            for i in range(latent.shape[1]):\n",
    "                output_step = self.decoder(latent[:, i].contiguous())\n",
    "                output.append(output_step)\n",
    "            return torch.cat(output, 1)\n",
    "        else:\n",
    "            return self.decoder(latent)\n",
    "    \n",
    "    def set_trainable(self, is_trainable):\n",
    "        self.encoder.set_trainable(is_trainable)\n",
    "        self.decoder.set_trainable(is_trainable)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.decode(self.encode(input))\n",
    "    \n",
    "    def get_loss(self, input, target, criterion, **kwargs):\n",
    "        return criterion(self(input), target)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        return self.encoder.get_regularization(source = source, mode = mode) + \\\n",
    "               self.decoder.get_regularization(source = source, mode = mode)\n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Conv_Autoencoder\"}\n",
    "        model_dict[\"net_type\"] = \"Conv_Autoencoder\"\n",
    "        model_dict[\"input_channels_encoder\"] = self.input_channels_encoder\n",
    "        model_dict[\"input_channels_decoder\"] = self.input_channels_decoder\n",
    "        model_dict[\"struct_param_encoder\"] = self.struct_param_encoder\n",
    "        model_dict[\"struct_param_decoder\"] = self.struct_param_decoder\n",
    "        model_dict[\"share_model_among_steps\"] = self.share_model_among_steps\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"encoder\"] = self.encoder.model_dict\n",
    "        model_dict[\"decoder\"] = self.decoder.model_dict\n",
    "        return model_dict\n",
    "    \n",
    "    def load_model_dict(self, model_dict):\n",
    "        model = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(model.__dict__)\n",
    "\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability models:\n",
    "### Mixture of Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixture_Gaussian(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_components,\n",
    "        dim,\n",
    "        param_mode = \"full\",\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Mixture_Gaussian, self).__init__()\n",
    "        self.num_components = num_components\n",
    "        self.dim = dim\n",
    "        self.param_mode = param_mode\n",
    "        self.device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "        self.info_dict = {}\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def initialize(self, input = None, num_samples = 100, verbose = False):\n",
    "        if input is not None:\n",
    "            neg_log_prob_min = np.inf\n",
    "            loc_init_min = None\n",
    "            scale_init_min = None\n",
    "            for i in range(num_samples):\n",
    "                neg_log_prob, loc_init_list, scale_init_list = self.initialize_ele(input)\n",
    "                if verbose:\n",
    "                    print(\"{0}: neg_log_prob: {1:.4f}\".format(i, neg_log_prob))\n",
    "                if neg_log_prob < neg_log_prob_min:\n",
    "                    neg_log_prob_min = neg_log_prob\n",
    "                    loc_init_min = loc_init_list\n",
    "                    scale_init_min = scale_init_list\n",
    "\n",
    "            for i in range(self.num_components):\n",
    "                setattr(self, \"loc_{0}\".format(i), nn.Parameter(loc_init_min[i].to(self.device)))\n",
    "                setattr(self, \"scale_{0}\".format(i), nn.Parameter(scale_init_min[i].to(self.device)))\n",
    "            print(\"min neg_log_prob: {0:.6f}\".format(to_np_array(neg_log_prob_min)))\n",
    "        else:\n",
    "            self.weight_logits = nn.Parameter((torch.randn(self.num_components) * np.sqrt(2 / (1 + self.dim))).to(self.device))\n",
    "            size = self.dim * (self.dim + 1) // 2\n",
    "            for i in range(self.num_components):\n",
    "                setattr(self, \"loc_{0}\".format(i), nn.Parameter(torch.randn(self.dim).to(self.device)))\n",
    "                setattr(self, \"scale_{0}\".format(i), nn.Parameter((torch.randn(1, size) / self.dim).to(self.device)))\n",
    "\n",
    "\n",
    "    def initialize_ele(self, input):\n",
    "        # Initialize the scale:\n",
    "        size = self.dim * (self.dim + 1) // 2\n",
    "        length = len(input)\n",
    "        self.weight_logits = nn.Parameter(torch.zeros(self.num_components).to(self.device))\n",
    "        loc_init_all = input[torch.multinomial(torch.ones(length) / length, self.num_components)].unsqueeze(0)\n",
    "        scale_init_list = []\n",
    "        loc_init_list = []\n",
    "        for i in range(self.num_components):\n",
    "            # Initialize the scale:\n",
    "            scale_init = torch.randn(1, size) * input.std() / 5\n",
    "            idx_list = []\n",
    "            for j in range(self.dim):\n",
    "                idx = (j + 1) * (j + 2) // 2 - 1\n",
    "                idx_list.append(idx)\n",
    "            diagonal = (input.std(0) / np.sqrt(2)) * (1 + torch.randn(self.dim) * 0.1)\n",
    "            diagonal = torch.log(torch.exp(diagonal) - 1)\n",
    "            scale_init[0, idx_list] = diagonal\n",
    "            scale_init_list.append(scale_init)\n",
    "            setattr(self, \"scale_{0}\".format(i), nn.Parameter(scale_init.to(self.device)))\n",
    "        \n",
    "            # Initialize the loc:\n",
    "            setattr(self, \"loc_{0}\".format(i), nn.Parameter(loc_init_all[:, i].to(self.device)))\n",
    "            loc_init_list.append(loc_init_all[:, i])\n",
    "        neg_log_prob = self.get_loss(input)\n",
    "        return neg_log_prob, loc_init_list, scale_init_list\n",
    "\n",
    "\n",
    "    def prob(self, input):\n",
    "        if len(input.shape) == 1:\n",
    "            input = input.unsqueeze(1)\n",
    "        assert len(input.shape) in [0, 2, 3]\n",
    "        prob_list = []\n",
    "        for i in range(self.num_components):\n",
    "            scale_tril = fill_triangular(getattr(self, \"scale_{0}\".format(i)), self.dim)\n",
    "            scale_tril = matrix_diag_transform(scale_tril, F.softplus)\n",
    "            dist = MultivariateNormal(getattr(self, \"loc_{0}\".format(i)), scale_tril = scale_tril)\n",
    "            setattr(self, \"component_{0}\".format(i), dist)\n",
    "            prob = torch.exp(dist.log_prob(input))\n",
    "            prob_list.append(prob)\n",
    "        prob_list = torch.stack(prob_list, -1)\n",
    "        prob = torch.matmul(prob_list, nn.Softmax(dim = 0)(self.weight_logits))\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        return torch.log(self.prob(input) + 1e-45)\n",
    "\n",
    "\n",
    "    def get_loss(self, X, y = None, **kwargs):\n",
    "        \"\"\"Optimize negative log-likelihood\"\"\"\n",
    "        neg_log_prob = - self.log_prob(X).mean() / np.log(2)\n",
    "        self.info_dict[\"loss\"] = to_np_array(neg_log_prob)\n",
    "        return neg_log_prob\n",
    "\n",
    "\n",
    "    def prepare_inspection(X, y, criterion, **kwargs):\n",
    "        return deepcopy(self.info_dict)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Mixture_Gaussian\"}\n",
    "        model_dict[\"num_components\"] = self.num_components\n",
    "        model_dict[\"dim\"] = self.dim\n",
    "        model_dict[\"weight_logits\"] = self.weight_logits\n",
    "        loc_list = []\n",
    "        scale_list = []\n",
    "        for i in range(self.num_components):\n",
    "            loc_list.append(to_np_array(getattr(self, \"loc_{0}\".format(i))))\n",
    "            scale_list.append(to_np_array(getattr(self, \"scale_{0}\".format(i))))\n",
    "        loc_list = np.stack(loc_list)\n",
    "        scale_list = np.concatenate(scale_list)\n",
    "        model_dict[\"loc_list\"] = loc_list\n",
    "        model_dict[\"scale_list\"] = scale_list\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def get_param(self):\n",
    "        weights = to_np_array(nn.Softmax(dim = 0)(self.weight_logits))\n",
    "        loc_list = self.model_dict[\"loc_list\"]\n",
    "        scale_list = self.model_dict[\"scale_list\"]\n",
    "        print(\"weights: {0}\".format(weights))\n",
    "        print(\"loc:\")\n",
    "        pp.pprint(loc_list)\n",
    "        print(\"scale:\")\n",
    "        pp.pprint(scale_list)\n",
    "        return weights, loc_list, scale_list\n",
    "\n",
    "\n",
    "    def visualize(self, input):\n",
    "        import scipy\n",
    "        import matplotlib.pylab as plt\n",
    "        std = to_np_array(input.std())\n",
    "        X = np.arange(to_np_array(input.min()) - 0.2 * std, to_np_array(input.max()) + 0.2 * std, 0.1)\n",
    "        Y_dict = {}\n",
    "        weights = nn.Softmax(dim = 0)(self.weight_logits)\n",
    "        plt.figure(figsize=(10, 4), dpi=100).set_facecolor('white')\n",
    "        for i in range(self.num_components):\n",
    "            Y_dict[i] = weights[0].item() * scipy.stats.norm.pdf((X - getattr(self, \"loc_{0}\".format(i)).item()) / getattr(self, \"scale_{0}\".format(i)).item())\n",
    "            plt.plot(X, Y_dict[i])\n",
    "        Y = np.sum([item for item in Y_dict.values()], 0)\n",
    "        plt.plot(X, Y, 'k--')\n",
    "        plt.plot(input.data.numpy(), np.zeros(len(input)), 'k*')\n",
    "        plt.title('Density of {0}-component mixture model'.format(self.num_components))\n",
    "        plt.ylabel('probability density');\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weights\", \"bias\"], mode = \"L1\", **kwargs):\n",
    "        reg = to_Variable([0], requires_grad = False).to(self.device)\n",
    "        return reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
