{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from pytorch_net.util import get_activation, init_weight, init_bias, init_module_weights, init_module_bias\n",
    "AVAILABLE_REG = [\"L1\", \"L2\", \"param\"]\n",
    "Default_Activation = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register all layer types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Layer(layer_type, input_size, output_size, W_init = None, b_init = None, settings = {}, is_cuda = False):\n",
    "    if layer_type == \"Simple_Layer\":\n",
    "        layer = Simple_Layer(input_size = input_size,\n",
    "                             output_size = output_size,\n",
    "                             W_init = W_init,\n",
    "                             b_init = b_init,\n",
    "                             settings = settings,\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "    elif layer_type == \"SuperNet_Layer\":\n",
    "        layer = SuperNet_Layer(input_size = input_size,\n",
    "                               output_size = output_size,\n",
    "                               W_init = W_init,\n",
    "                               b_init = b_init,\n",
    "                               settings = settings,\n",
    "                               is_cuda = is_cuda,\n",
    "                              )   \n",
    "    else:\n",
    "        raise Exception(\"layer_type '{0}' not recognized!\".format(layer_type))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def load_layer_dict(layer_dict, layer_type, is_cuda = False):\n",
    "    new_layer = get_Layer(layer_type = \"Symbolic_Layer\",\n",
    "                          input_size = layer_dict[\"input_size\"],\n",
    "                          output_size = layer_dict[\"output_size\"],\n",
    "                          W_init = layer_dict[\"weights\"],\n",
    "                          b_init = layer_dict[\"bias\"],\n",
    "                          settings = layer_dict[\"settings\"],\n",
    "                          is_cuda = is_cuda,\n",
    "                         )\n",
    "    return new_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        W_init = None,     # initialization for weights\n",
    "        b_init = None,     # initialization for bias\n",
    "        settings = {},     # Other settings that are relevant to this specific layer\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        # Firstly, must perform this step:\n",
    "        super(Simple_Layer, self).__init__()\n",
    "        # Saving the attribuites:\n",
    "        if isinstance(input_size, tuple):\n",
    "            self.input_size = reduce(lambda x, y: x * y, input_size)\n",
    "            self.input_size_original = input_size\n",
    "        else:\n",
    "            self.input_size = input_size\n",
    "        if isinstance(output_size, tuple):\n",
    "            self.output_size = reduce(lambda x, y: x * y, output_size)\n",
    "            self.output_size_original = output_size\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "        # self.W_init, self.b_init can be a numpy array, or a string like \"glorot-normal\":\n",
    "        self.W_init = W_init\n",
    "        self.b_init = b_init\n",
    "        self.is_cuda = is_cuda\n",
    "        self.settings = settings\n",
    "        \n",
    "        # Other attributes that are specific to this layer:\n",
    "        self.activation = settings[\"activation\"] if \"activation\" in settings else Default_Activation\n",
    "        \n",
    "        # Define the learnable parameters in the module (use any name you like). \n",
    "        # use nn.Parameter() so that the parameters is registered in the module and can be gradient-updated:\n",
    "        self.W_core = nn.Parameter(torch.randn(self.input_size, self.output_size))\n",
    "        self.b_core = nn.Parameter(torch.zeros(self.output_size))\n",
    "        # Use the given W_init (numpy array or string) to initialize the weights:\n",
    "        init_weight(self.W_core, init = self.W_init)  \n",
    "        init_bias(self.b_core, init = self.b_init)\n",
    "        if is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        output_size = self.output_size_original if hasattr(self, \"output_size_original\") else self.output_size\n",
    "        return [output_size, \"Simple_Layer\", self.settings]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def layer_dict(self):\n",
    "        input_size = self.input_size_original if hasattr(self, \"input_size_original\") else self.input_size\n",
    "        output_size = self.output_size_original if hasattr(self, \"output_size_original\") else self.output_size\n",
    "        Layer_dict =  {\n",
    "            \"input_size\": input_size,\n",
    "            \"output_size\": output_size,\n",
    "            \"settings\": self.settings,\n",
    "        }\n",
    "        Layer_dict[\"weights\"], Layer_dict[\"bias\"] = self.get_weights_bias()\n",
    "        return Layer_dict\n",
    "\n",
    "\n",
    "    def load_layer_dict(self, layer_dict):\n",
    "        new_layer = load_layer_dict(layer_dict, \"Simple_Layer\", self.is_cuda)\n",
    "        self.__dict__.update(new_layer.__dict__)\n",
    "\n",
    "\n",
    "    def forward(self, input, p_dict = None):\n",
    "        output = input\n",
    "        if hasattr(self, \"input_size_original\"):\n",
    "            output = output.view(-1, self.input_size)\n",
    "        # Perform dot(X, W) + b:\n",
    "        output = torch.matmul(output, self.W_core) + self.b_core\n",
    "        \n",
    "        # If p_dict is not None, update the first neuron's activation according to p_dict:\n",
    "        if p_dict is not None:\n",
    "            p_dict = p_dict.view(-1)\n",
    "            if len(p_dict) == 2:\n",
    "                output_0 = output[:,:1] * p_dict[1] + p_dict[0]\n",
    "            elif len(p_dict) == 1:\n",
    "                output_0 = output[:,:1] + p_dict[0]\n",
    "            else:\n",
    "                raise\n",
    "            if output.size(1) > 1:\n",
    "                output = torch.cat([output_0, output[:,1:]], 1)\n",
    "            else:\n",
    "                output = output_0\n",
    "\n",
    "        # Perform activation function:\n",
    "        output = get_activation(self.activation)(output)\n",
    "        if hasattr(self, \"output_size_original\"):\n",
    "            output = output.view(*((-1,) + self.output_size_original))\n",
    "        assert output.size(0) == input.size(0), \"output_size {0} must have same length as input_size {1}. Check shape!\".format(output.size(0), input.size(0))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def prune_output_neurons(self, neuron_ids):\n",
    "        if not isinstance(neuron_ids, list):\n",
    "            neuron_ids = [neuron_ids]\n",
    "        preserved_ids = torch.LongTensor(np.array(list(set(range(self.output_size)) - set(neuron_ids))))\n",
    "        if self.is_cuda:\n",
    "            preserved_ids = preserved_ids.cuda()\n",
    "        self.W_core = nn.Parameter(self.W_core.data[:, preserved_ids])\n",
    "        self.b_core = nn.Parameter(self.b_core.data[preserved_ids])\n",
    "        self.output_size = self.W_core.size(1)\n",
    "    \n",
    "    \n",
    "    def prune_input_neurons(self, neuron_ids):\n",
    "        if not isinstance(neuron_ids, list):\n",
    "            neuron_ids = [neuron_ids]\n",
    "        preserved_ids = torch.LongTensor(np.array(list(set(range(self.input_size)) - set(neuron_ids))))\n",
    "        self.W_core = nn.Parameter(self.W_core.data[preserved_ids, :])\n",
    "        self.input_size = self.W_core.size(0)\n",
    "    \n",
    "    \n",
    "    def add_output_neurons(self, num_neurons, mode = \"imitation\"):\n",
    "        if mode == \"imitation\":\n",
    "            W_core_mean = self.W_core.mean().data[0]\n",
    "            W_core_std = self.W_core.std().data[0]\n",
    "            b_core_mean = self.b_core.mean().data[0]\n",
    "            b_core_std = self.b_core.std().data[0]\n",
    "            new_W_core = torch.randn(self.input_size, num_neurons) * W_core_std + W_core_mean\n",
    "            new_b_core = torch.randn(num_neurons) * b_core_std + b_core_mean\n",
    "        elif mode == \"zeros\":\n",
    "            new_W_core = torch.zeros(self.input_size, num_neurons)\n",
    "            new_b_core = torch.zeros(num_neurons)\n",
    "        else:\n",
    "            raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        self.W_core = nn.Parameter(torch.cat([self.W_core.data, new_W_core], 1))\n",
    "        self.b_core = nn.Parameter(torch.cat([self.b_core.data, new_b_core], 0))\n",
    "        self.output_size += num_neurons\n",
    "        \n",
    "    \n",
    "    def add_input_neurons(self, num_neurons, mode = \"imitation\"):\n",
    "        if mode == \"imitation\":\n",
    "            W_core_mean = self.W_core.mean().data[0]\n",
    "            W_core_std = self.W_core.std().data[0]\n",
    "            b_core_mean = self.b_core.mean().data[0]\n",
    "            b_core_std = self.b_core.std().data[0]\n",
    "            new_W_core = torch.randn(num_neurons, self.output_size) * W_core_std + W_core_mean\n",
    "        elif mode == \"zeros\":\n",
    "            new_W_core = torch.zeros(num_neurons, self.output_size)\n",
    "        else:\n",
    "            raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        self.W_core = nn.Parameter(torch.cat([self.W_core.data, new_W_core], 0))\n",
    "        self.input_size += num_neurons\n",
    "\n",
    "\n",
    "    \n",
    "    def get_weights_bias(self):\n",
    "        W_core, b_core = self.W_core, self.b_core\n",
    "        if self.is_cuda:\n",
    "            W_core = W_core.cpu()\n",
    "            b_core = b_core.cpu()\n",
    "        return deepcopy(W_core.data.numpy()), deepcopy(b_core.data.numpy())\n",
    "\n",
    "    \n",
    "    def get_regularization(self, mode, source = [\"weight\", \"bias\"]):\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for source_ele in source:\n",
    "            if source_ele == \"weight\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.W_core.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.W_core ** 2).sum()\n",
    "                elif mode in AVAILABLE_REG:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\"mode '{0}' not recognized!\".format(mode))\n",
    "            elif source_ele == \"bias\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.b_core.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.b_core ** 2).sum()\n",
    "                elif mode in AVAILABLE_REG:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\"mode '{0}' not recognized!\".format(mode))\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        if is_cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        if is_trainable:\n",
    "            self.W_core.requires_grad = True\n",
    "            self.b_core.requires_grad = True\n",
    "        else:\n",
    "            self.W_core.requires_grad = False\n",
    "            self.b_core.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperNet_Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        W_init = None,     # initialization for weights\n",
    "        b_init = None,     # initialization for bias\n",
    "        settings = {},\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(SuperNet_Layer, self).__init__()\n",
    "        # Saving the attribuites:\n",
    "        if isinstance(input_size, tuple):\n",
    "            self.input_size = reduce(lambda x, y: x * y, input_size)\n",
    "            self.input_size_original = input_size\n",
    "        else:\n",
    "            self.input_size = input_size\n",
    "        if isinstance(output_size, tuple):\n",
    "            self.output_size = reduce(lambda x, y: x * y, output_size)\n",
    "            self.output_size_original = output_size\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "        self.W_init = W_init\n",
    "        self.b_init = b_init\n",
    "        self.is_cuda = is_cuda\n",
    "        self.device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "        \n",
    "        # Obtain additional initialization settings if provided:\n",
    "        self.settings = settings\n",
    "        self.W_available = settings[\"W_available\"] if \"W_available\" in settings else [\"dense\", \"Toeplitz\"]\n",
    "        self.b_available = settings[\"b_available\"] if \"b_available\" in settings else [\"dense\", \"None\"]\n",
    "        self.A_available = settings[\"A_available\"] if \"A_available\" in settings else [\"linear\", \"relu\"]\n",
    "        self.W_sig_init  = settings[\"W_sig_init\"] if \"W_sig_init\" in settings else None # initialization for the significance for the weights\n",
    "        self.b_sig_init  = settings[\"b_sig_init\"] if \"b_sig_init\" in settings else None # initialization for the significance for the bias\n",
    "        self.A_sig_init  = settings[\"A_sig_init\"] if \"A_sig_init\" in settings else None # initialization for the significance for the activations\n",
    "        for W_candidate in self.W_available:\n",
    "            if \"2D-in\" in W_candidate:\n",
    "                self.input_size_2D = settings[\"input_size_2D\"]\n",
    "            if \"2D-out\" in W_candidate:\n",
    "                self.output_size_2D = settings[\"output_size_2D\"]\n",
    "        for b_candidate in self.b_available:\n",
    "            if \"2D\" in b_candidate:\n",
    "                self.output_size_2D = settings[\"output_size_2D\"]\n",
    "        \n",
    "        # Initialize layer:\n",
    "        self.init_layer()\n",
    "        if is_cuda:\n",
    "            self.cuda()\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [self.output_size, \"SuperNet_Layer\", self.settings]\n",
    "\n",
    "        \n",
    "    def init_layer(self):\n",
    "        self.W_layer_seed = nn.Parameter(torch.FloatTensor(np.random.randn(self.input_size, self.output_size)))\n",
    "        self.b_layer_seed = nn.Parameter(torch.zeros(self.output_size))\n",
    "        init_weight(self.W_layer_seed, init = self.W_init)\n",
    "        init_bias(self.b_layer_seed, init = self.b_init)\n",
    "        if \"arithmetic-series-in\" in self.W_available:\n",
    "            self.W_interval_j = nn.Parameter(torch.randn(self.output_size) / np.sqrt(self.input_size + self.output_size))\n",
    "        if \"arithmetic-series-out\" in self.W_available:\n",
    "            self.W_interval_i = nn.Parameter(torch.randn(self.input_size) / np.sqrt(self.input_size + self.output_size))\n",
    "        if \"arithmetic-series-2D-in\" in self.W_available:\n",
    "            self.W_mean_2D_in = nn.Parameter(torch.randn(self.output_size) / np.sqrt(self.input_size_2D[0] + self.input_size_2D[1] + self.output_size))\n",
    "            self.W_interval_2D_in = nn.Parameter(torch.randn(2, self.output_size) / np.sqrt(self.input_size_2D[0] + self.input_size_2D[1] + self.output_size))\n",
    "        if \"arithmetic-series-2D-out\" in self.W_available:\n",
    "            self.W_mean_2D_out = nn.Parameter(torch.randn(self.input_size) / np.sqrt(self.input_size + self.output_size_2D[0] + self.output_size_2D[1]))\n",
    "            self.W_interval_2D_out = nn.Parameter(torch.randn(2, self.input_size) / np.sqrt(self.input_size + self.output_size_2D[0] + self.output_size_2D[1]))\n",
    "        if \"arithmetic-series\" in self.b_available:\n",
    "            self.b_interval = nn.Parameter(torch.randn(1) / np.sqrt(self.output_size))\n",
    "        if \"arithmetic-series-2D\" in self.b_available:\n",
    "            self.b_mean_2D = nn.Parameter(torch.randn(1) / np.sqrt(self.output_size))\n",
    "            self.b_interval_2D = nn.Parameter(torch.randn(2) / np.sqrt(self.output_size_2D[0] + self.output_size_2D[1]))\n",
    "        \n",
    "        if self.W_sig_init is None:\n",
    "            self.W_sig = nn.Parameter(torch.zeros(len(self.W_available)))\n",
    "        else:\n",
    "            self.W_sig = nn.Parameter(torch.FloatTensor(self.W_sig_init))\n",
    "        if self.b_sig_init is None:\n",
    "            self.b_sig = nn.Parameter(torch.zeros(len(self.b_available)))\n",
    "        else:\n",
    "            self.b_sig = nn.Parameter(torch.FloatTensor(self.b_sig_init))\n",
    "        if self.A_sig_init is None:\n",
    "            self.A_sig = nn.Parameter(torch.zeros(len(self.A_available)))\n",
    "        else:\n",
    "            self.A_sig = nn.Parameter(torch.FloatTensor(self.A_sig_init))\n",
    "\n",
    "\n",
    "    def get_layers(self, source = [\"weight\", \"bias\"]):\n",
    "        \"\"\"All the different SuperNet layers are based on the same W_seed matrices. \n",
    "        For example, W_seed is based on the full self.W_layer_seed; \"Toeplitz\" is based on\n",
    "        the first row and first column of self.W_layer_seed to construct the Toeplitz matrix, etc.\n",
    "        \"\"\"\n",
    "        # Superimpose different weights:\n",
    "        if \"weight\" in source:\n",
    "            self.W_list = []\n",
    "            for weight_type in self.W_available:\n",
    "                if weight_type == \"dense\":\n",
    "                    W_layer = self.W_layer_seed\n",
    "                elif weight_type == \"Toeplitz\":\n",
    "                    W_layer_stacked = []\n",
    "                    if self.output_size > 1:\n",
    "                        inv_idx = torch.arange(self.output_size - 1, 0, -1).long().to(self.device)\n",
    "                        W_seed = torch.cat([self.W_layer_seed[0][inv_idx], self.W_layer_seed[:,0]])\n",
    "                    else:\n",
    "                        W_seed = self.W_layer_seed[:,0]\n",
    "                    for j in range(self.output_size):\n",
    "                        W_layer_stacked.append(W_seed[self.output_size - j - 1: self.output_size - j - 1 + self.input_size])\n",
    "                    W_layer = torch.stack(W_layer_stacked, 1)\n",
    "                elif weight_type == \"arithmetic-series-in\":\n",
    "                    mean_j = self.W_layer_seed.mean(0)\n",
    "                    idx_i = torch.FloatTensor(np.repeat(np.arange(self.input_size), self.output_size)).to(self.device)\n",
    "                    idx_j = torch.LongTensor(range(self.output_size) * self.input_size).to(self.device)\n",
    "                    offset = self.input_size / float(2) - 0.5\n",
    "                    W_layer = (mean_j[idx_j] + self.W_interval_j[idx_j] * Variable(idx_i - offset, requires_grad = False)).view(self.input_size, self.output_size)\n",
    "                elif weight_type == \"arithmetic-series-out\":\n",
    "                    mean_i = self.W_layer_seed.mean(1)\n",
    "                    idx_i = torch.LongTensor(np.repeat(np.arange(self.input_size), self.output_size)).to(self.device)\n",
    "                    idx_j = torch.FloatTensor(range(self.output_size) * self.input_size).to(self.device)\n",
    "                    offset = self.output_size / float(2) - 0.5\n",
    "                    W_layer = (mean_i[idx_i] + self.W_interval_i[idx_i] * Variable(idx_j - offset, requires_grad = False)).view(self.input_size, self.output_size)\n",
    "                elif weight_type == \"arithmetic-series-2D-in\":\n",
    "                    idx_i, idx_j, idx_k = np.meshgrid(range(self.input_size_2D[0]), range(self.input_size_2D[1]), range(self.output_size), indexing = \"ij\")\n",
    "                    idx_i = torch.from_numpy(idx_i).float().view(-1).to(self.device)\n",
    "                    idx_j = torch.from_numpy(idx_j).float().view(-1).to(self.device)\n",
    "                    idx_k = torch.from_numpy(idx_k).long().view(-1).to(self.device)\n",
    "                    offset_i = self.input_size_2D[0] / float(2) - 0.5\n",
    "                    offset_j = self.input_size_2D[1] / float(2) - 0.5\n",
    "                    W_layer = (self.W_mean_2D_in[idx_k] + \\\n",
    "                               self.W_interval_2D_in[:, idx_k][0] * Variable(idx_i - offset_i, requires_grad = False) + \\\n",
    "                               self.W_interval_2D_in[:, idx_k][1] * Variable(idx_j - offset_j, requires_grad = False)).view(self.input_size, self.output_size)\n",
    "                elif weight_type == \"arithmetic-series-2D-out\":\n",
    "                    idx_k, idx_i, idx_j = np.meshgrid(range(self.input_size), range(self.output_size_2D[0]), range(self.output_size_2D[1]), indexing = \"ij\")\n",
    "                    idx_k = torch.from_numpy(idx_k).long().view(-1).to(self.device)\n",
    "                    idx_i = torch.from_numpy(idx_i).float().view(-1).to(self.device)\n",
    "                    idx_j = torch.from_numpy(idx_j).float().view(-1).to(self.device)\n",
    "                    offset_i = self.output_size_2D[0] / float(2) - 0.5\n",
    "                    offset_j = self.output_size_2D[1] / float(2) - 0.5\n",
    "                    W_layer = (self.W_mean_2D_out[idx_k] + \\\n",
    "                               self.W_interval_2D_out[:, idx_k][0] * Variable(idx_i - offset_i, requires_grad = False) + \\\n",
    "                               self.W_interval_2D_out[:, idx_k][1] * Variable(idx_j - offset_j, requires_grad = False)).view(self.input_size, self.output_size)\n",
    "                else:\n",
    "                    raise Exception(\"weight_type '{0}' not recognized!\".format(weight_type))\n",
    "                self.W_list.append(W_layer)\n",
    "\n",
    "            if len(self.W_available) == 1:\n",
    "                self.W_core = W_layer\n",
    "            else:\n",
    "                self.W_list = torch.stack(self.W_list, dim = 2)\n",
    "                W_sig_softmax = nn.Softmax(dim = -1)(self.W_sig.unsqueeze(0))\n",
    "                self.W_core = torch.matmul(self.W_list, W_sig_softmax.transpose(1,0)).squeeze(2)\n",
    "    \n",
    "        # Superimpose different biases:\n",
    "        if \"bias\" in source:\n",
    "            self.b_list = []\n",
    "            for bias_type in self.b_available:\n",
    "                if bias_type == \"None\":\n",
    "                    b_layer = Variable(torch.zeros(self.output_size).to(self.device), requires_grad = False)\n",
    "                elif bias_type == \"constant\":\n",
    "                    b_layer = self.b_layer_seed[0].repeat(self.output_size)\n",
    "                elif bias_type == \"arithmetic-series\":\n",
    "                    mean = self.b_layer_seed.mean()\n",
    "                    offset = self.output_size / float(2) - 0.5\n",
    "                    idx = Variable(torch.FloatTensor(range(self.output_size)).to(self.device), requires_grad = False)\n",
    "                    b_layer = mean + self.b_interval * (idx - offset)\n",
    "                elif bias_type == \"arithmetic-series-2D\":\n",
    "                    idx_i, idx_j = np.meshgrid(range(self.output_size_2D[0]), range(self.output_size_2D[1]), indexing = \"ij\")\n",
    "                    idx_i = torch.from_numpy(idx_i).float().view(-1).to(self.device)\n",
    "                    idx_j = torch.from_numpy(idx_j).float().view(-1).to(self.device)\n",
    "                    offset_i = self.output_size_2D[0] / float(2) - 0.5\n",
    "                    offset_j = self.output_size_2D[1] / float(2) - 0.5\n",
    "                    b_layer = (self.b_mean_2D + \\\n",
    "                               self.b_interval_2D[0] * Variable(idx_i - offset_i, requires_grad = False) + \\\n",
    "                               self.b_interval_2D[1] * Variable(idx_j - offset_j, requires_grad = False)).view(-1)\n",
    "                elif bias_type == \"dense\":\n",
    "                    b_layer = self.b_layer_seed\n",
    "                else:\n",
    "                    raise Exception(\"bias_type '{0}' not recognized!\".format(bias_type))\n",
    "                self.b_list.append(b_layer)\n",
    "\n",
    "            if len(self.b_available) == 1:\n",
    "                self.b_core = b_layer\n",
    "            else:\n",
    "                self.b_list = torch.stack(self.b_list, dim = 1)\n",
    "                b_sig_softmax = nn.Softmax(dim = -1)(self.b_sig.unsqueeze(0))\n",
    "                self.b_core = torch.matmul(self.b_list, b_sig_softmax.transpose(1,0)).squeeze(1)\n",
    "\n",
    "\n",
    "    def forward(self, X, p_dict = None):\n",
    "        del p_dict\n",
    "        output = X\n",
    "        if hasattr(self, \"input_size_original\"):\n",
    "            output = output.view(-1, self.input_size)\n",
    "        # Get superposition of layers:\n",
    "        self.get_layers(source = [\"weight\", \"bias\"])\n",
    "\n",
    "        # Perform dot(X, W) + b:\n",
    "        output = torch.matmul(output, self.W_core) + self.b_core\n",
    "        \n",
    "        # Exert superposition of activation functions:\n",
    "        if len(self.A_available) == 1:\n",
    "            output = get_activation(self.A_available[0])(output)\n",
    "        else:\n",
    "            self.A_list = []\n",
    "            A_sig_softmax = nn.Softmax(dim = -1)(self.A_sig.unsqueeze(0))\n",
    "            for i, activation in enumerate(self.A_available):\n",
    "                A = get_activation(activation)(output)\n",
    "                self.A_list.append(A)\n",
    "            self.A_list = torch.stack(self.A_list, 2)\n",
    "            output = torch.matmul(self.A_list, A_sig_softmax.transpose(1,0)).squeeze(2)\n",
    "\n",
    "        if hasattr(self, \"output_size_original\"):\n",
    "            output = output.view(*((-1,) + self.output_size_original))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_param_names(self, source):\n",
    "        if source == \"modules\":\n",
    "            param_names = [\"W_layer_seed\", \"b_layer_seed\"]\n",
    "            if \"arithmetic-series-in\" in self.W_available:\n",
    "                param_names.append(\"W_interval_j\")\n",
    "            if \"arithmetic-series-out\" in self.W_available:\n",
    "                param_names.append(\"W_interval_i\")\n",
    "            if \"arithmetic-series-2D-in\" in self.W_available:\n",
    "                param_names = param_names + [\"W_mean_2D_in\", \"W_interval_2D_in\"]\n",
    "            if \"arithmetic-series-2D-out\" in self.W_available:\n",
    "                param_names = param_names + [\"W_mean_2D_out\", \"W_interval_2D_out\"]\n",
    "            if \"arithmetic-series\" in self.b_available:\n",
    "                param_names.append(\"b_interval\")\n",
    "        if source == \"attention\":\n",
    "            param_names = [\"W_sig\", \"b_sig\", \"A_sig\"]\n",
    "        return param_names\n",
    "    \n",
    "    \n",
    "    def get_weights_bias(self):\n",
    "        self.get_layers(source = [\"weight\", \"bias\"])\n",
    "        return to_np_array(self.W_core), to_np_array(self.b_core)\n",
    "\n",
    "\n",
    "    def get_regularization(self, mode, source = [\"weight\", \"bias\"]):\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False).to(self.device)\n",
    "        if not isinstance(source, list):\n",
    "            source = [source]\n",
    "        if mode == \"L1\":\n",
    "            if \"weight\" in source:\n",
    "                reg = reg + self.W_core.abs().sum()\n",
    "            if \"bias\" in source:\n",
    "                reg = reg + self.b_core.abs().sum()\n",
    "        elif mode == \"layer_L1\":\n",
    "            if \"weight\" in source:\n",
    "                self.get_layers(source = [\"weight\"])\n",
    "                reg = reg + self.W_list.abs().sum()\n",
    "            if \"bias\" in source:\n",
    "                self.get_layers(source = [\"bias\"])\n",
    "                reg = reg + self.b_list.abs().sum()\n",
    "        elif mode == \"L2\":\n",
    "            if \"weight\" in source:\n",
    "                reg = reg + torch.sum(self.W_core ** 2)\n",
    "            if \"bias\" in source:\n",
    "                reg = reg + torch.sum(self.b_core ** 2)\n",
    "        elif mode == \"S_entropy\":\n",
    "            if \"weight\" in source:\n",
    "                W_sig_softmax = nn.Softmax(dim = -1)(self.W_sig.unsqueeze(0))\n",
    "                reg = reg - torch.sum(W_sig_softmax * torch.log(W_sig_softmax))\n",
    "            if \"bias\" in source:\n",
    "                b_sig_softmax = nn.Softmax(dim = -1)(self.b_sig.unsqueeze(0))\n",
    "                reg = reg - torch.sum(b_sig_softmax * torch.log(b_sig_softmax))\n",
    "        elif mode == \"S_entropy_activation\":\n",
    "            A_sig_softmax = nn.Softmax(dim = -1)(self.A_sig.unsqueeze(0))\n",
    "            reg = reg - torch.sum(A_sig_softmax * torch.log(A_sig_softmax))\n",
    "        elif mode in AVAILABLE_REG:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"mode '{0}' not recognized!\".format(mode))\n",
    "        return reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
