{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from pytorch_net.util import get_activation, init_weight, init_bias, init_module_weights, init_module_bias\n",
    "AVAILABLE_REG = [\"L1\", \"L2\", \"param\"]\n",
    "Default_Activation = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register all layer types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Layer(layer_type, input_size, output_size, W_init = None, b_init = None, settings = {}, is_cuda = False):\n",
    "    if layer_type == \"Simple_Layer\":\n",
    "        layer = Simple_Layer(input_size = input_size,\n",
    "                             output_size = output_size,\n",
    "                             W_init = W_init,\n",
    "                             b_init = b_init,\n",
    "                             settings = settings,\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "    else:\n",
    "        raise Exception(\"layer_type '{0}' not recognized!\".format(layer_type))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def load_layer_dict(layer_dict, layer_type, is_cuda = False):\n",
    "    new_layer = get_Layer(layer_type = \"Symbolic_Layer\",\n",
    "                          input_size = layer_dict[\"input_size\"],\n",
    "                          output_size = layer_dict[\"output_size\"],\n",
    "                          W_init = layer_dict[\"weights\"],\n",
    "                          b_init = layer_dict[\"bias\"],\n",
    "                          settings = layer_dict[\"settings\"],\n",
    "                          is_cuda = is_cuda,\n",
    "                         )\n",
    "    return new_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        W_init = None,     # initialization for weights\n",
    "        b_init = None,     # initialization for bias\n",
    "        settings = {},     # Other settings that are relevant to this specific layer\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        # Firstly, must perform this step:\n",
    "        super(Simple_Layer, self).__init__()\n",
    "        # Saving the attribuites:\n",
    "        if isinstance(input_size, tuple):\n",
    "            self.input_size = reduce(lambda x, y: x * y, input_size)\n",
    "            self.input_size_original = input_size\n",
    "        else:\n",
    "            self.input_size = input_size\n",
    "        if isinstance(output_size, tuple):\n",
    "            self.output_size = reduce(lambda x, y: x * y, output_size)\n",
    "            self.output_size_original = output_size\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "        # self.W_init, self.b_init can be a numpy array, or a string like \"glorot-normal\":\n",
    "        self.W_init = W_init\n",
    "        self.b_init = b_init\n",
    "        self.is_cuda = is_cuda\n",
    "        self.settings = settings\n",
    "        \n",
    "        # Other attributes that are specific to this layer:\n",
    "        self.activation = settings[\"activation\"] if \"activation\" in settings else Default_Activation\n",
    "        \n",
    "        # Define the learnable parameters in the module (use any name you like). \n",
    "        # use nn.Parameter() so that the parameters is registered in the module and can be gradient-updated:\n",
    "        self.W_core = nn.Parameter(torch.randn(self.input_size, self.output_size))\n",
    "        self.b_core = nn.Parameter(torch.zeros(self.output_size))\n",
    "        # Use the given W_init (numpy array or string) to initialize the weights:\n",
    "        init_weight(self.W_core, init = self.W_init)  \n",
    "        init_bias(self.b_core, init = self.b_init)\n",
    "        if is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        output_size = self.output_size_original if hasattr(self, \"output_size_original\") else self.output_size\n",
    "        return [output_size, \"Simple_Layer\", self.settings]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def layer_dict(self):\n",
    "        input_size = self.input_size_original if hasattr(self, \"input_size_original\") else self.input_size\n",
    "        output_size = self.output_size_original if hasattr(self, \"output_size_original\") else self.output_size\n",
    "        Layer_dict =  {\n",
    "            \"input_size\": input_size,\n",
    "            \"output_size\": output_size,\n",
    "            \"settings\": self.settings,\n",
    "        }\n",
    "        Layer_dict[\"weights\"], Layer_dict[\"bias\"] = self.get_weights_bias()\n",
    "        return Layer_dict\n",
    "\n",
    "\n",
    "    def load_layer_dict(self, layer_dict):\n",
    "        new_layer = load_layer_dict(layer_dict, \"Simple_Layer\", self.is_cuda)\n",
    "        self.__dict__.update(new_layer.__dict__)\n",
    "\n",
    "\n",
    "    def forward(self, input, p_dict = None):\n",
    "        output = input\n",
    "        if hasattr(self, \"input_size_original\"):\n",
    "            output = output.view(-1, self.input_size)\n",
    "        # Perform dot(X, W) + b:\n",
    "        output = torch.matmul(output, self.W_core) + self.b_core\n",
    "        \n",
    "        # If p_dict is not None, update the first neuron's activation according to p_dict:\n",
    "        if p_dict is not None:\n",
    "            p_dict = p_dict.view(-1)\n",
    "            if len(p_dict) == 2:\n",
    "                output_0 = output[:,:1] * p_dict[1] + p_dict[0]\n",
    "            elif len(p_dict) == 1:\n",
    "                output_0 = output[:,:1] + p_dict[0]\n",
    "            else:\n",
    "                raise\n",
    "            if output.size(1) > 1:\n",
    "                output = torch.cat([output_0, output[:,1:]], 1)\n",
    "            else:\n",
    "                output = output_0\n",
    "\n",
    "        # Perform activation function:\n",
    "        output = get_activation(self.activation)(output)\n",
    "        if hasattr(self, \"output_size_original\"):\n",
    "            output = output.view(*((-1,) + self.output_size_original))\n",
    "        assert output.size(0) == input.size(0), \"output_size {0} must have same length as input_size {1}. Check shape!\".format(output.size(0), input.size(0))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def prune_output_neurons(self, neuron_ids):\n",
    "        if not isinstance(neuron_ids, list):\n",
    "            neuron_ids = [neuron_ids]\n",
    "        preserved_ids = torch.LongTensor(np.array(list(set(range(self.output_size)) - set(neuron_ids))))\n",
    "        if self.is_cuda:\n",
    "            preserved_ids = preserved_ids.cuda()\n",
    "        self.W_core = nn.Parameter(self.W_core.data[:, preserved_ids])\n",
    "        self.b_core = nn.Parameter(self.b_core.data[preserved_ids])\n",
    "        self.output_size = self.W_core.size(1)\n",
    "    \n",
    "    \n",
    "    def prune_input_neurons(self, neuron_ids):\n",
    "        if not isinstance(neuron_ids, list):\n",
    "            neuron_ids = [neuron_ids]\n",
    "        preserved_ids = torch.LongTensor(np.array(list(set(range(self.input_size)) - set(neuron_ids))))\n",
    "        self.W_core = nn.Parameter(self.W_core.data[preserved_ids, :])\n",
    "        self.input_size = self.W_core.size(0)\n",
    "    \n",
    "    \n",
    "    def add_output_neurons(self, num_neurons, mode = \"imitation\"):\n",
    "        if mode == \"imitation\":\n",
    "            W_core_mean = self.W_core.mean().data[0]\n",
    "            W_core_std = self.W_core.std().data[0]\n",
    "            b_core_mean = self.b_core.mean().data[0]\n",
    "            b_core_std = self.b_core.std().data[0]\n",
    "            new_W_core = torch.randn(self.input_size, num_neurons) * W_core_std + W_core_mean\n",
    "            new_b_core = torch.randn(num_neurons) * b_core_std + b_core_mean\n",
    "        elif mode == \"zeros\":\n",
    "            new_W_core = torch.zeros(self.input_size, num_neurons)\n",
    "            new_b_core = torch.zeros(num_neurons)\n",
    "        else:\n",
    "            raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        self.W_core = nn.Parameter(torch.cat([self.W_core.data, new_W_core], 1))\n",
    "        self.b_core = nn.Parameter(torch.cat([self.b_core.data, new_b_core], 0))\n",
    "        self.output_size += num_neurons\n",
    "        \n",
    "    \n",
    "    def add_input_neurons(self, num_neurons, mode = \"imitation\"):\n",
    "        if mode == \"imitation\":\n",
    "            W_core_mean = self.W_core.mean().data[0]\n",
    "            W_core_std = self.W_core.std().data[0]\n",
    "            b_core_mean = self.b_core.mean().data[0]\n",
    "            b_core_std = self.b_core.std().data[0]\n",
    "            new_W_core = torch.randn(num_neurons, self.output_size) * W_core_std + W_core_mean\n",
    "        elif mode == \"zeros\":\n",
    "            new_W_core = torch.zeros(num_neurons, self.output_size)\n",
    "        else:\n",
    "            raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        self.W_core = nn.Parameter(torch.cat([self.W_core.data, new_W_core], 0))\n",
    "        self.input_size += num_neurons\n",
    "\n",
    "\n",
    "    \n",
    "    def get_weights_bias(self):\n",
    "        W_core, b_core = self.W_core, self.b_core\n",
    "        if self.is_cuda:\n",
    "            W_core = W_core.cpu()\n",
    "            b_core = b_core.cpu()\n",
    "        return deepcopy(W_core.data.numpy()), deepcopy(b_core.data.numpy())\n",
    "\n",
    "    \n",
    "    def get_regularization(self, mode, source = [\"weight\", \"bias\"]):\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for source_ele in source:\n",
    "            if source_ele == \"weight\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.W_core.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.W_core ** 2).sum()\n",
    "                elif mode in AVAILABLE_REG:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\"mode '{0}' not recognized!\".format(mode))\n",
    "            elif source_ele == \"bias\":\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + self.b_core.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (self.b_core ** 2).sum()\n",
    "                elif mode in AVAILABLE_REG:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\"mode '{0}' not recognized!\".format(mode))\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def set_cuda(self, is_cuda):\n",
    "        if is_cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "\n",
    "    def set_trainable(self, is_trainable):\n",
    "        if is_trainable:\n",
    "            self.W_core.requires_grad = True\n",
    "            self.b_core.requires_grad = True\n",
    "        else:\n",
    "            self.W_core.requires_grad = False\n",
    "            self.b_core.requires_grad = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
